#include "peano/utils/Loop.h"
#include "tarch/Assertions.h"
#include "peano/geometry/GeometryHelper.h"


#include <set>


#ifdef Parallel
#include <mpi.h>

#include "tarch/parallel/NodePool.h"
#include "peano/parallel/AdjacencyList.h"
#include "peano/parallel/SendReceiveBufferPool.h"
#include "peano/parallel/JoinDataBufferPool.h"
#include "peano/parallel/messages/LoadBalancingMessage.h"
#include "peano/parallel/loadbalancing/OracleForOnePhase.h"
#include "peano/parallel/loadbalancing/Oracle.h"
#include "peano/grid/aspects/VertexStateAnalysis.h"
#include "peano/grid/aspects/ParallelMerge.h"
#endif


#include "peano/grid/aspects/VertexStateAnalysis.h"


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
tarch::logging::Log peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::_log( "peano::grid::nodes::Node" );


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::Node(
  VertexStack&                vertexStack,
  CellStack&                  cellStack,
  EventHandle&                eventHandle,
  peano::geometry::Geometry&  geometry
):
  _vertexStack(vertexStack),
  _cellStack(cellStack),
  _eventHandle(eventHandle),
  _geometry(geometry) {
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::~Node() {
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
CellStack& peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::getCellStack() const {
  return _cellStack;
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
VertexStack& peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::getVertexStack() const {
  return _vertexStack;
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::validatePositionOfVertices(
  Vertex                                                 fineGridVertices[FOUR_POWER_D],
  const peano::grid::VertexEnumerator&  fineGridVerticesEnumerator
) {
  logTraceIn( "validatePositionOfVertices(...)" );
  #if defined(Asserts) && defined(Debug)
  dfor2(k)
    assertionEquals3(
      fineGridVerticesEnumerator.getLevel(),
      fineGridVertices[fineGridVerticesEnumerator(k)].getLevel(),
      fineGridVerticesEnumerator.toString(),
      fineGridVertices[fineGridVerticesEnumerator(k)],
      k
    );
    for (int d=0; d<DIMENSIONS; d++) {
      assertionNumericalEquals4(
        fineGridVerticesEnumerator.getVertexPosition(k)(d),
        fineGridVertices[fineGridVerticesEnumerator(k)].getX()(d),
        fineGridVerticesEnumerator.toString(),
        fineGridVertices[fineGridVerticesEnumerator(k)],
        k,d
      )
    }
  enddforx
  #endif
  logTraceOut( "validatePositionOfVertices(...)" );
}


#ifdef Parallel
template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::makeCellRemoteCell(
  State&                         state,
  int                            remoteRank,
  Cell&                          fineGridCell,
  Vertex                         fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&   fineGridVerticesEnumerator
) const {
  logTraceInWith4Arguments( "makeCellRemoteCell(...)", remoteRank, fineGridCell, fineGridVerticesEnumerator.toString(), tarch::parallel::Node::getInstance().getRank() );

  fineGridCell.assignToRemoteNode( remoteRank );
  peano::parallel::AdjacencyListAspect<Vertex>::replaceAdjancyEntriesOfVerticesOfOneCellWithDifferentRank(remoteRank,fineGridVertices,fineGridVerticesEnumerator);
  state.changedCellState();

  dfor2(j)
    logDebug( "makeCellRemoteCell(...)", "- vertex: " << fineGridVertices[fineGridVerticesEnumerator(j)].toString() );
  enddforx

  logTraceOut( "makeCellRemoteCell(...)" );
}
#endif



template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::updateCellsParallelStateBeforeStoreForRootOfDeployedSubtree(
  State&                                    state,
  Cell&                                     fineGridCell,
  Vertex                                    fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              fineGridVerticesEnumerator,
  Cell&                                     coarseGridCell,
  Vertex                                    coarseGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              coarseGridVerticesEnumerator,
  const tarch::la::Vector<DIMENSIONS,int>&  fineGridPositionOfCell
) const {
  #ifdef Parallel
    const int currentWorker = fineGridCell.getRankOfRemoteNode();
    logTraceInWith3Arguments( "updateCellsParallelStateBeforeStoreForRootOfDeployedSubtree(...)", state.toString(), fineGridCell.toString(), currentWorker );

    peano::parallel::loadbalancing::Oracle::getInstance().masterStartsToWaitForWorkers();

    Cell    receivedWorkerCell;
    Vertex  receivedWorkerVertices[TWO_POWER_D];
    receivedWorkerCell.receive(currentWorker,peano::parallel::SendReceiveBufferPool::getInstance().getIterationManagementTag(),true);
    for (int i=0; i<TWO_POWER_D; i++) {
      receivedWorkerVertices[i].receive(currentWorker,peano::parallel::SendReceiveBufferPool::getInstance().getIterationManagementTag(),true);

      #ifdef Debug
      assertionVectorNumericalEquals5(
        receivedWorkerVertices[i].getX(),
        fineGridVertices[ fineGridVerticesEnumerator(i) ].getX(),
        receivedWorkerVertices[i].toString(),
        fineGridVertices[ fineGridVerticesEnumerator(i) ].toString(),
        i,
        fineGridVerticesEnumerator.toString(),
        tarch::parallel::Node::getInstance().getRank()
      );
      #endif
    }

    State workerState;
    workerState.receive(currentWorker,peano::parallel::SendReceiveBufferPool::getInstance().getIterationManagementTag());
    state.mergeWithWorkerState(workerState);
    peano::parallel::loadbalancing::Oracle::getInstance().setWorkerHasWorker(currentWorker,workerState.hasWorker());

    _eventHandle.mergeWithMaster(
      receivedWorkerCell,
      receivedWorkerVertices,
      fineGridCell,
      fineGridVertices,
      fineGridVerticesEnumerator,
      coarseGridVertices,
      coarseGridVerticesEnumerator,
      coarseGridCell,
      fineGridPositionOfCell,
      currentWorker,
      workerState,
      state
    );

    peano::parallel::loadbalancing::Oracle::getInstance().receivedTerminateCommand(
      currentWorker,
      workerState.getNumberOfInnerCells()
    );

    tarch::parallel::Node::getInstance().ensureThatMessageQueuesAreEmpty(currentWorker,peano::parallel::SendReceiveBufferPool::getInstance().getIterationManagementTag());

    logTraceOutWith1Argument( "updateCellsParallelStateBeforeStoreForRootOfDeployedSubtree(...)", state.toString() );

  #endif
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::updateCellsParallelStateBeforeStore(
  State&                                    state,
  Cell&                                     fineGridCell,
  Vertex                                    fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              fineGridVerticesEnumerator,
  Cell&                                     coarseGridCell,
  Vertex                                    coarseGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              coarseGridVerticesEnumerator,
  const tarch::la::Vector<DIMENSIONS,int>&  fineGridPositionOfCell
) const {
  #ifdef Parallel
  if (
    state.reduceStateAndCell() &&
    fineGridCell.isRemote(state,false,false) &&
    !coarseGridCell.isRemote(state,false,false)
  ) {
    updateCellsParallelStateBeforeStoreForRootOfDeployedSubtree(
      state,
      fineGridCell,
      fineGridVertices,
      fineGridVerticesEnumerator,
      coarseGridCell,
      coarseGridVertices,
      coarseGridVerticesEnumerator,
      fineGridPositionOfCell
    );
  }
  else if (
    fineGridCell.isRemote(state,false,true) &&
    !coarseGridCell.isRemote(state,false,true) &&
    state.isJoiningRank(fineGridCell.getRankOfRemoteNode())
  ) {
    logTraceInWith2Arguments( "updateCellsParallelStateBeforeStore(...)", state.toString(), fineGridCell.toString() );

    peano::parallel::loadbalancing::Oracle::getInstance().removeWorker(fineGridCell.getRankOfRemoteNode());

    logTraceOutWith1Argument( "updateCellsParallelStateBeforeStore(...)", state.toString() );
  }

  if (
    fineGridCell.isRemote(state,false,true) &&
    state.isJoiningRank(fineGridCell.getRankOfRemoteNode())
  ) {
    fineGridCell.assignToLocalNode();
  }
  #endif
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::updateCellsParallelStateAfterLoadForRootOfDeployedSubtree(
  State&                                    state,
  Cell&                                     fineGridCell,
  Vertex                                    fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              fineGridVerticesEnumerator,
  Cell&                                     coarseGridCell,
  Vertex                                    coarseGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              coarseGridVerticesEnumerator,
  const tarch::la::Vector<DIMENSIONS,int>&  fineGridPositionOfCell
) const {
  #ifdef Parallel
    logTraceInWith2Arguments( "updateCellsParallelStateAfterLoadForRootOfDeployedSubtree(...)", state.toString(), fineGridCell.toString() );
    dfor2(k)
      logDebug( "updateCellsParallelStateAfterLoadForRootOfDeployedSubtree(...)", "- adjacent vertex: " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() );
    enddforx

    const int currentWorker = fineGridCell.getRankOfRemoteNode();
    logDebug( "updateCellsParallelStateAfterLoadForRootOfDeployedSubtree()", "startup worker " << currentWorker );

    peano::parallel::messages::LoadBalancingMessage loadBalancingMessage;
    loadBalancingMessage.setLoadBalancingFlag(
      peano::parallel::loadbalancing::Oracle::getInstance().getCommandForWorker(
        currentWorker,
        false,
        state.mayForkDueToLoadBalancing(currentWorker),
        state.mayJoinDueToLoadBalancing(currentWorker)
      )
    );
    if (loadBalancingMessage.getLoadBalancingFlag()==peano::parallel::loadbalancing::Join) {
      state.joinWithRank(currentWorker);

      peano::parallel::JoinDataBufferPool::getInstance().createVertexBufferManually<Vertex>( true, currentWorker );
      peano::parallel::JoinDataBufferPool::getInstance().createCellBufferManually<Cell>( true, currentWorker );
    }
    if (loadBalancingMessage.getLoadBalancingFlag()>=peano::parallel::loadbalancing::ForkOnce) {
      peano::parallel::loadbalancing::Oracle::getInstance().setWorkerHasWorker(currentWorker,true);
    }

    logDebug( "updateCellsParallelStateAfterLoadForRootOfDeployedSubtree()", "send balancing message " << loadBalancingMessage.toString() << " to rank " << currentWorker );
    loadBalancingMessage.send(
      currentWorker,
      peano::parallel::SendReceiveBufferPool::getInstance().getIterationManagementTag(),
      true
    );

    logDebug( "updateCellsParallelStateAfterLoadForRootOfDeployedSubtree()", "send state " << state.toString() << " to rank " << currentWorker );
    state.send(
      currentWorker,
      peano::parallel::SendReceiveBufferPool::getInstance().getIterationManagementTag()
    );

    _eventHandle.prepareSendToWorker(
      fineGridCell,
      fineGridVertices,
      fineGridVerticesEnumerator,
      coarseGridVertices,
      coarseGridVerticesEnumerator,
      coarseGridCell,
      fineGridPositionOfCell,
      currentWorker
    );

    logDebug( "updateCellsParallelStateAfterLoadForRootOfDeployedSubtree()", "send cell " << fineGridCell.toString() << " to rank " << currentWorker );
    fineGridCell.send(currentWorker,peano::parallel::SendReceiveBufferPool::getInstance().getIterationManagementTag(),true);
    dfor2(k)
      logDebug( "updateCellsParallelStateAfterLoadForRootOfDeployedSubtree()", "send vertex " << fineGridVertices[ fineGridVerticesEnumerator(k) ].toString() << " to rank " << currentWorker );
      fineGridVertices[ fineGridVerticesEnumerator(k) ].send(currentWorker,peano::parallel::SendReceiveBufferPool::getInstance().getIterationManagementTag(),true);
    enddforx
    logTraceOut( "updateCellsParallelStateAfterLoadForRootOfDeployedSubtree(...)" );
  #endif
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::updateCellsParallelStateAfterLoadIfStateIsForking(
  State&                                    state,
  Cell&                                     fineGridCell,
  Vertex                                    fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              fineGridVerticesEnumerator,
  Cell&                                     coarseGridCell,
  Vertex                                    coarseGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              coarseGridVerticesEnumerator,
  const tarch::la::Vector<DIMENSIONS,int>&  fineGridPositionOfCell
) const {
  #ifdef Parallel
    logTraceInWith2Arguments( "updateCellsParallelStateAfterLoadIfStateIsForking(...)", state.toString(), fineGridCell.toString() );
    dfor2(k)
      logDebug( "updateCellsParallelStateAfterLoadIfStateIsForking(...)", "- adjacent vertex: " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() );
    enddforx
    const std::set<int> forkingRanks= state.getForkingOrJoiningOrTriggeredForRebalancingRanks();
    for (std::set<int>::const_iterator p = forkingRanks.begin(); p!=forkingRanks.end();p++ ) {
      const std::bitset<NUMBER_OF_VERTICES_PER_ELEMENT> adjacencyBitset   = peano::grid::aspects::VertexStateAnalysis::whichPersistentVerticesAreAdjacentToRank(*p,fineGridVertices,fineGridVerticesEnumerator);
      if (adjacencyBitset.any()) {
        _eventHandle.prepareCopyToRemoteNode(fineGridCell,*p);

        peano::parallel::JoinDataBufferPool::getInstance().sendCell(fineGridCell,adjacencyBitset,*p);

        if (
          fineGridCell.isInside() &&
          fineGridCell.isAssignedToRemoteRank()
        ) {
          _eventHandle.destroyCell(
            fineGridCell,
            fineGridVertices,
            fineGridVerticesEnumerator,
            coarseGridVertices,
            coarseGridVerticesEnumerator,
            coarseGridCell,
            fineGridPositionOfCell
          );
          fineGridCell.switchToOutside();
          state.changedCellState();
        }

        logDebug( "updateCellsParallelStateAfterLoadIfStateIsForking()", "due to fork sent cell " << fineGridCell.toString() << " with flag (" << adjacencyBitset << ") to rank " << *p );
        dfor2(k)
          if (adjacencyBitset[kScalar]) {
            // I have to invalidate the information, as the store process might delete or switch to outside
            // this vertex after this iteration. And if I switch a vertex to outside, it has to be invalidated
            // before (see assertion).
            fineGridVertices[ fineGridVerticesEnumerator(k) ].invalidateAdjacentCellInformation();
            _eventHandle.prepareCopyToRemoteNode(fineGridVertices[fineGridVerticesEnumerator(k)],*p);
            peano::parallel::JoinDataBufferPool::getInstance().sendVertex(fineGridVertices[fineGridVerticesEnumerator(k)],*p);
            logDebug( "updateCellsParallelStateAfterLoadIfStateIsForking()", "due to fork sent vertex " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() << " to rank " << *p );
            // any destruction of a vertex due to a fork is done in StoreVertexLoopBody
            // as it should be destructed after 'all' forked nodes have received it, not
            // earlier
          }
        enddforx
      }
    }
    logTraceOut( "updateCellsParallelStateAfterLoadIfStateIsForking(...)" );
  #endif
}



template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::updateCellsParallelStateAfterLoadIfNodeIsJoiningWithWorker(
  State&                                    state,
  Cell&                                     fineGridCell,
  Vertex                                    fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              fineGridVerticesEnumerator,
  Cell&                                     coarseGridCell,
  Vertex                                    coarseGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              coarseGridVerticesEnumerator,
  const tarch::la::Vector<DIMENSIONS,int>&  fineGridPositionOfCell
) const {
  #ifdef Parallel
    logTraceInWith2Arguments( "updateCellsParallelStateAfterLoadIfNodeIsJoiningWithWorker(...)", state.toString(), fineGridCell.toString() );
    dfor2(k)
      logDebug( "updateCellsParallelStateAfterLoadIfNodeIsJoiningWithWorker(...)", "- adjacent vertex: " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() );
    enddforx
    const std::set<int> joiningRanks= state.getForkingOrJoiningOrTriggeredForRebalancingRanks();
    for (std::set<int>::const_iterator p = joiningRanks.begin(); p!=joiningRanks.end();p++ ) {
      // is done twice, but obviously it is necessary as it might already be deleted again
      peano::parallel::JoinDataBufferPool::getInstance().createVertexBufferManually<Vertex>( true, *p );
      peano::parallel::JoinDataBufferPool::getInstance().createCellBufferManually<Cell>( true, *p );

      const std::bitset<NUMBER_OF_VERTICES_PER_ELEMENT> adjacencyBitset   = peano::grid::aspects::VertexStateAnalysis::whichPersistentVerticesAreAdjacentToRank(*p,fineGridVertices,fineGridVerticesEnumerator);
      if (adjacencyBitset.any()) {
        #if defined(Debug) && (defined(Dim2) || defined(Dim3) || defined(Dim4))
        const std::bitset<NUMBER_OF_VERTICES_PER_ELEMENT> receivedBitset( peano::parallel::JoinDataBufferPool::getInstance().getCellMarkerFromStream(*p) );
        assertionEquals3(
          receivedBitset,
          adjacencyBitset,
          fineGridVerticesEnumerator.toString(),
          *p,
          adjacencyBitset
        );
        assertionEquals3(
          fineGridCell.getLevel(),
          peano::parallel::JoinDataBufferPool::getInstance().getCellLevelFromStream(*p),
          fineGridVerticesEnumerator.toString(),
          *p,
          adjacencyBitset
        );
        #endif
        logDebug( "updateCellsParallelStateAfterLoadIfNodeIsJoiningWithWorker(...)", "received flag " << adjacencyBitset << " due to join with worker");
        peano::parallel::JoinDataBufferPool::getInstance().removeCellMarkerFromStream(*p,true);

        const Cell receivedCell = peano::parallel::JoinDataBufferPool::getInstance().getCellFromStream<Cell>(*p);
        if (
          peano::grid::aspects::ParallelMerge::mergeWithJoinedCellFromWorker(
            fineGridCell,
            receivedCell,
            *p
          )
        ) {
          _eventHandle.createCell(
            fineGridCell,
            fineGridVertices,
            fineGridVerticesEnumerator,
            coarseGridVertices,
            coarseGridVerticesEnumerator,
            coarseGridCell,
            fineGridPositionOfCell
          );
          fineGridCell.switchToInside();
          state.changedCellState();
        }

        _eventHandle.mergeWithRemoteDataDueToForkOrJoin(fineGridCell,receivedCell,*p,fineGridVerticesEnumerator.getCellCenter(),fineGridVerticesEnumerator.getCellSize(),fineGridVerticesEnumerator.getLevel());
        peano::parallel::JoinDataBufferPool::getInstance().removeCellFromStream(*p);
        logDebug( "updateCellsParallelStateAfterLoadIfNodeIsJoiningWithWorker(...)", "received and merged cell " << fineGridCell.toString()  << " due to join with worker");

        dfor2(k)
          if (adjacencyBitset[kScalar]) {
            fineGridVertices[ fineGridVerticesEnumerator(k) ].invalidateAdjacentCellInformation();
            const Vertex receivedVertex = peano::parallel::JoinDataBufferPool::getInstance().getVertexFromStream<Vertex>(*p);
            const peano::grid::aspects::ParallelMerge::MergeVertexDueToJoinEffect modifyVertex =
              peano::grid::aspects::ParallelMerge::mergeWithJoinedVertexFromWorker(
                fineGridVertices[fineGridVerticesEnumerator(k)],
                receivedVertex
              );
            switch (modifyVertex) {
              case peano::grid::aspects::ParallelMerge::CreateBoundaryVertexOnMaster:
                _eventHandle.createBoundaryVertex(
                  fineGridVertices[ fineGridVerticesEnumerator(k) ],
                  fineGridVerticesEnumerator.getVertexPosition(fineGridPositionOfCell+k),
                  fineGridVerticesEnumerator.getCellSize(),
                  coarseGridVertices,
                  coarseGridVerticesEnumerator,
                  coarseGridCell,
                  fineGridPositionOfCell + k
                );
                fineGridVertices[ fineGridVerticesEnumerator(k) ].switchToBoundary();
                state.updateRefinementHistoryAfterLoad( false, false, true );
                break;
              case peano::grid::aspects::ParallelMerge::CreateInnerVertexOnMaster:
                _eventHandle.createInnerVertex(
                  fineGridVertices[ fineGridVerticesEnumerator(k) ],
                  fineGridVerticesEnumerator.getVertexPosition(fineGridPositionOfCell+k),
                  fineGridVerticesEnumerator.getCellSize(),
                  coarseGridVertices,
                  coarseGridVerticesEnumerator,
                  coarseGridCell,
                  fineGridPositionOfCell + k
                );
                fineGridVertices[ fineGridVerticesEnumerator(k) ].switchToInside();
                state.updateRefinementHistoryAfterLoad( false, false, true );
                break;
              case peano::grid::aspects::ParallelMerge::MasterVertexStateRemainsUnaltered:
                break;
            }
            _eventHandle.mergeWithRemoteDataDueToForkOrJoin(fineGridVertices[fineGridVerticesEnumerator(k)],receivedVertex,*p,fineGridVerticesEnumerator.getVertexPosition(k),fineGridVerticesEnumerator.getCellSize(),fineGridVerticesEnumerator.getLevel());
            peano::parallel::JoinDataBufferPool::getInstance().removeVertexFromStream(*p);
            logDebug( "updateCellsParallelStateAfterLoadIfNodeIsJoiningWithWorker(...)", "received and merged vertex " << fineGridVertices[fineGridVerticesEnumerator(k)].toString()  << " due to join with worker");
          }
        enddforx
      }
    }
    logTraceOut( "updateCellsParallelStateAfterLoadIfNodeIsJoiningWithWorker(...)" );
  #endif
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::updateCellsParallelStateAfterLoadForNewWorkerDueToForkOfExistingDomain(
  State&                                    state,
  Cell&                                     fineGridCell,
  Vertex                                    fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              fineGridVerticesEnumerator,
  Cell&                                     coarseGridCell,
  Vertex                                    coarseGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              coarseGridVerticesEnumerator,
  const tarch::la::Vector<DIMENSIONS,int>&  fineGridPositionOfCell
) const {
  #ifdef Parallel
    logTraceInWith2Arguments( "updateCellsParallelStateAfterLoad(...)", state.toString(), fineGridCell.toString() );
    dfor2(k)
      logDebug( "updateCellsParallelStateAfterLoad(...)", "- adjacent vertex: " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() );
    enddforx
    const std::bitset<NUMBER_OF_VERTICES_PER_ELEMENT> localAdjacencyBitset = peano::grid::aspects::VertexStateAnalysis::whichPersistentVerticesAreAdjacentToRank(tarch::parallel::Node::getInstance().getRank(),fineGridVertices,fineGridVerticesEnumerator);
    if (localAdjacencyBitset.any()) {
      const std::bitset<NUMBER_OF_VERTICES_PER_ELEMENT> adjacencyBitset( peano::parallel::JoinDataBufferPool::getInstance().getCellMarkerFromStream(tarch::parallel::NodePool::getInstance().getMasterRank()) );

      #if defined(Debug) && (defined(Dim2) || defined(Dim3) || defined(Dim4))
      assertionEquals2(
        fineGridCell.getLevel(),
        peano::parallel::JoinDataBufferPool::getInstance().getCellLevelFromStream(tarch::parallel::NodePool::getInstance().getMasterRank()),
        fineGridVerticesEnumerator.toString(),
        tarch::parallel::NodePool::getInstance().getMasterRank()
      );
      #endif
      assertionEquals4(localAdjacencyBitset,adjacencyBitset,localAdjacencyBitset,adjacencyBitset,fineGridVerticesEnumerator.toString(),tarch::parallel::NodePool::getInstance().getMasterRank());

      logDebug( "updateCellsParallelStateAfterLoad(...)", "received flag " << adjacencyBitset );
      peano::parallel::JoinDataBufferPool::getInstance().removeCellMarkerFromStream(tarch::parallel::NodePool::getInstance().getMasterRank(),false);

      const Cell receivedCell = peano::parallel::JoinDataBufferPool::getInstance().getCellFromStream<Cell>(tarch::parallel::NodePool::getInstance().getMasterRank());
      peano::grid::aspects::ParallelMerge::mergeWithForkedCellFromMaster(
        fineGridCell,
        receivedCell
      );
      _eventHandle.mergeWithRemoteDataDueToForkOrJoin(fineGridCell,receivedCell,tarch::parallel::NodePool::getInstance().getMasterRank(),fineGridVerticesEnumerator.getCellCenter(),fineGridVerticesEnumerator.getCellSize(),fineGridVerticesEnumerator.getLevel());
      peano::parallel::JoinDataBufferPool::getInstance().removeCellFromStream(tarch::parallel::NodePool::getInstance().getMasterRank());
      logDebug( "updateCellsParallelStateAfterLoad(...)", "received and merged cell " << fineGridCell.toString() );

      dfor2(k)
        if (adjacencyBitset[kScalar]) {
          const Vertex receivedVertex = peano::parallel::JoinDataBufferPool::getInstance().getVertexFromStream<Vertex>(tarch::parallel::NodePool::getInstance().getMasterRank());;
          peano::grid::aspects::ParallelMerge::mergeWithForkedVertexFromMaster(
            fineGridVertices[fineGridVerticesEnumerator(k)],
            receivedVertex
          );
          _eventHandle.mergeWithRemoteDataDueToForkOrJoin(fineGridVertices[fineGridVerticesEnumerator(k)],receivedVertex,tarch::parallel::NodePool::getInstance().getMasterRank(),fineGridVerticesEnumerator.getVertexPosition(k),fineGridVerticesEnumerator.getCellSize(),fineGridVerticesEnumerator.getLevel());
          peano::parallel::JoinDataBufferPool::getInstance().removeVertexFromStream(tarch::parallel::NodePool::getInstance().getMasterRank());
          logDebug( "updateCellsParallelStateAfterLoad(...)", "received and merged vertex " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() );
        }
      enddforx
    }
    logTraceOut( "updateCellsParallelStateAfterLoad(...)" );
  #endif
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::updateCellsParallelStateAfterLoad(
  State&                                    state,
  Cell&                                     fineGridCell,
  Vertex                                    fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              fineGridVerticesEnumerator,
  Cell&                                     coarseGridCell,
  Vertex                                    coarseGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              coarseGridVerticesEnumerator,
  const tarch::la::Vector<DIMENSIONS,int>&  fineGridPositionOfCell
) const {
  #ifdef Parallel
  if (
    fineGridCell.isRemote(state,true,true) &&
    !coarseGridCell.isRemote(state,true,true) &&
    !state.isJoiningRank(fineGridCell.getRankOfRemoteNode())
  ) {
    updateCellsParallelStateAfterLoadForRootOfDeployedSubtree(
      state,
      fineGridCell,
      fineGridVertices,
      fineGridVerticesEnumerator,
      coarseGridCell,
      coarseGridVertices,
      coarseGridVerticesEnumerator,
      fineGridPositionOfCell
    );
  }
  else if (!fineGridCell.isAssignedToRemoteRank() && coarseGridCell.isAssignedToRemoteRank() && !coarseGridCell.isRoot()) {
    logTraceInWith3Arguments( "updateCellsParallelStateAfterLoad(...)", state.toString(), fineGridCell.toString(), coarseGridCell.toString() );
    dfor2(k)
      logDebug( "updateCellsParallelStateAfterLoad(...)", "- adjacent vertex: " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() );
    enddforx
    assertion4(
      state.isForkTriggered() || state.isNewWorkerDueToForkOfExistingDomain(),
      state.toString(),
      fineGridCell.toString(), coarseGridCell.toString(),
      tarch::parallel::Node::getInstance().getRank()
    );
    makeCellRemoteCell(state,coarseGridCell.getRankOfRemoteNode(),fineGridCell,fineGridVertices,fineGridVerticesEnumerator);
    logDebug( "updateCellsParallelStateAfterLoad()", "made local cell remote and updated its adjacent vertices. cell =  " << fineGridCell.toString() );
    logTraceOut( "updateCellsParallelStateAfterLoad(...)" );
  }

  if (state.isForking()) {
    updateCellsParallelStateAfterLoadIfStateIsForking(
      state,
      fineGridCell,
      fineGridVertices,
      fineGridVerticesEnumerator,
      coarseGridCell,
      coarseGridVertices,
      coarseGridVerticesEnumerator,
      fineGridPositionOfCell
    );
  }
  else if (state.isJoiningWithWorker()) {
    updateCellsParallelStateAfterLoadIfNodeIsJoiningWithWorker(
      state,
      fineGridCell,
      fineGridVertices,
      fineGridVerticesEnumerator,
      coarseGridCell,
      coarseGridVertices,
      coarseGridVerticesEnumerator,
      fineGridPositionOfCell
    );
  }
  else if (state.isJoiningWithMaster()) {
    logTraceInWith2Arguments( "updateCellsParallelStateAfterLoad(...)", state.toString(), fineGridCell.toString() );
    dfor2(k)
      logDebug( "updateCellsParallelStateAfterLoad(...)", "- adjacent vertex: " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() );
    enddforx
    const std::bitset<NUMBER_OF_VERTICES_PER_ELEMENT> adjacencyBitset   = peano::grid::aspects::VertexStateAnalysis::whichPersistentVerticesAreAdjacentToRank(tarch::parallel::Node::getInstance().getRank(),fineGridVertices,fineGridVerticesEnumerator);
    if (adjacencyBitset.any()) {
      _eventHandle.prepareCopyToRemoteNode(fineGridCell,tarch::parallel::NodePool::getInstance().getMasterRank());
      peano::parallel::JoinDataBufferPool::getInstance().sendCell(fineGridCell,adjacencyBitset,tarch::parallel::NodePool::getInstance().getMasterRank());

      _eventHandle.destroyCell(
        fineGridCell,
        fineGridVertices,
        fineGridVerticesEnumerator,
        coarseGridVertices,
        coarseGridVerticesEnumerator,
        coarseGridCell,
        fineGridPositionOfCell
      );

      logDebug( "updateCellsParallelStateAfterLoad()", "due to join sent cell " << fineGridCell.toString() << " with flag (" << adjacencyBitset << ") to rank " << tarch::parallel::NodePool::getInstance().getMasterRank() );
      dfor2(k)
        if (adjacencyBitset[kScalar]) {
          fineGridVertices[ fineGridVerticesEnumerator(k) ].invalidateAdjacentCellInformation();
          _eventHandle.prepareCopyToRemoteNode(fineGridVertices[fineGridVerticesEnumerator(k)],tarch::parallel::NodePool::getInstance().getMasterRank());
          peano::parallel::JoinDataBufferPool::getInstance().sendVertex(fineGridVertices[fineGridVerticesEnumerator(k)],tarch::parallel::NodePool::getInstance().getMasterRank());
           logDebug( "updateCellsParallelStateAfterLoad()", "due to join sent vertex " << fineGridVertices[fineGridVerticesEnumerator(k)].toString() << " to rank " << tarch::parallel::NodePool::getInstance().getMasterRank() );
        }
      enddforx
    }
    logTraceOut( "updateCellsParallelStateAfterLoad(...)" );
  }
  else if (state.isNewWorkerDueToForkOfExistingDomain() ) {
    updateCellsParallelStateAfterLoadForNewWorkerDueToForkOfExistingDomain(
      state,
      fineGridCell,
      fineGridVertices,
      fineGridVerticesEnumerator,
      coarseGridCell,
      coarseGridVertices,
      coarseGridVerticesEnumerator,
      fineGridPositionOfCell
    );
  }
  #endif
}


template <class Vertex, class Cell, class State, class VertexStack, class CellStack, class EventHandle>
void peano::grid::nodes::Node<Vertex,Cell,State,VertexStack,CellStack,EventHandle>::updateCellsGeometryInformationAfterLoad(
  State&                                    state,
  Cell&                                     fineGridCell,
  Vertex                                    fineGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              fineGridVerticesEnumerator,
  Cell&                                     coarseGridCell,
  Vertex                                    coarseGridVertices[FOUR_POWER_D],
  const SingleLevelEnumerator&              coarseGridVerticesEnumerator,
  const tarch::la::Vector<DIMENSIONS,int>&  fineGridPositionOfCell
) {
  logTraceInWith2Arguments( "updateCellsGeometryInformationAfterLoad(...)", state.toString(), fineGridCell.toString() );

  #ifdef Parallel
  const bool studyCell                   = (fineGridVerticesEnumerator.getCellFlags()<=peano::grid::NotStationary) && !fineGridCell.isRemote(state,false,false);
  #else
  const bool studyCell                   = fineGridVerticesEnumerator.getCellFlags()<=peano::grid::NotStationary;
  #endif

  const SingleLevelEnumerator::Vector cellCenter      = peano::geometry::GeometryHelper::getCellCenter(fineGridVerticesEnumerator.getVertexPosition(),fineGridVerticesEnumerator.getCellSize());
  const SingleLevelEnumerator::Vector halfTheCellSize = fineGridVerticesEnumerator.getCellSize()/2.0;

  if ( studyCell ) {
    switch (
      peano::geometry::GeometryHelper::getCellCommand(
        _geometry.isCompletelyInside( cellCenter,halfTheCellSize ),
        _geometry.isCompletelyOutside( cellCenter,halfTheCellSize ),
        fineGridCell.isInside()
      )
    ) {
      case peano::geometry::GeometryHelper::LeaveCellUnaltered:
        break;
      case peano::geometry::GeometryHelper::CreateInnerCell:
        _eventHandle.createCell(
          fineGridCell,
          fineGridVertices,
          fineGridVerticesEnumerator,
          coarseGridVertices,
          coarseGridVerticesEnumerator,
          coarseGridCell,
          fineGridPositionOfCell
        );
        fineGridCell.switchToInside();
        state.changedCellState();
        logDebug( "updateCellsGeometryInformationAfterLoad()", "switch cell to inside. cell=" << fineGridCell );
        break;
      case peano::geometry::GeometryHelper::CreateOuterCell:
      case peano::geometry::GeometryHelper::CreateOuterCellAndDoNotAnalyseItFurtherIfItsRefined:
        _eventHandle.destroyCell(
          fineGridCell,
          fineGridVertices,
          fineGridVerticesEnumerator,
          coarseGridVertices,
          coarseGridVerticesEnumerator,
          coarseGridCell,
          fineGridPositionOfCell
        );
        fineGridCell.switchToOutside();
        state.changedCellState();
        logDebug( "updateCellsGeometryInformationAfterLoad()", "switch cell to outside. cell=" << fineGridCell );
        break;
    }
  }


  #ifdef Parallel
  const bool checkWhetherToRefineOutCell = (fineGridVerticesEnumerator.getCellFlags()<=peano::grid::NotStationary) && !fineGridCell.isRemote(state,false,false) && fineGridCell.isOutside();
  #else
  const bool checkWhetherToRefineOutCell = fineGridVerticesEnumerator.getCellFlags()<=peano::grid::NotStationary && fineGridCell.isOutside();
  #endif

  if (checkWhetherToRefineOutCell) {
    if (
      peano::grid::aspects::VertexStateAnalysis::doAllVerticesCarryRefinementFlag(
        fineGridVertices,
        fineGridVerticesEnumerator,
        Vertex::Records::Unrefined
      ) &&
      !_geometry.isCompletelyOutside(cellCenter,fineGridVerticesEnumerator.getCellSize()) &&
      _geometry.refineOuterCellWithExclusivelyOuterVerticesAsItIntersectsDomain(cellCenter,halfTheCellSize)
    ) {
      dfor2(k)
        if (
          !fineGridVertices[ fineGridVerticesEnumerator(k) ].isHangingNode()
        ) {
          fineGridVertices[ fineGridVerticesEnumerator(k) ].refine();
        }
      enddforx
    }
  }

  logTraceOutWith2Arguments( "updateCellsGeometryInformationAfterLoad(...)", state.toString(), fineGridCell.toString() );
}
