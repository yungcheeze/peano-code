#include "tarch/Assertions.h"


template<class Data>
tarch::logging::Log  peano::heap::BoundaryDataExchanger<Data>::_log( "peano::heap::BoundaryDataExchanger" );



template<class Data>
peano::heap::BoundaryDataExchanger<Data>::BoundaryDataExchanger():
  _identifier( "created-by-standard-constructor"),
  _tag(-1),
  _rank(-1),
  _numberOfSentMessages(-1),
  _numberOfSentRecords(-1),
  _numberOfReceivedMessages(-1),
  _numberOfReceivedRecords(-1),
  _currentReceiveBuffer(-1),
  _readDeployBufferInReverseOrder(false),
  _wasTraversalInvertedThroughoutLastSendReceiveTraversal(false) {
}


template<class Data>
peano::heap::BoundaryDataExchanger<Data>::BoundaryDataExchanger(
  const std::string& identifier,
  int tag,
  int rank
):
  _identifier(identifier),
  _tag(tag),
  _rank(rank),
  _numberOfSentMessages(0),
  _numberOfSentRecords(0),
  _numberOfReceivedMessages(0),
  _numberOfReceivedRecords(0),
  _currentReceiveBuffer(0),
  _readDeployBufferInReverseOrder(false),
  _wasTraversalInvertedThroughoutLastSendReceiveTraversal(false)
  #ifdef Asserts
  ,_isCurrentlySending(true)
  #endif
  {
}


template<class Data>
void peano::heap::BoundaryDataExchanger<Data>::startToSendData(bool isTraversalInverted) {
  assertion(_receiveTasks[1-_currentReceiveBuffer].empty());
  assertion(_currentReceiveBuffer>=0);
  assertion(_currentReceiveBuffer<=1);

  #ifdef Asserts
  _isCurrentlySending = true;
  #endif

  const int numberOfSentMessages = releaseSentMessages();

  // @todo umbauen
  logInfo(
    "startToSendData(int)",
    "sent " << numberOfSentMessages <<
    " message(s)"
  );

  waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages(numberOfSentMessages);

  releaseReceivedNeighbourMessagesRequests();

  assertion3(
    _receiveTasks[0].empty() || _receiveTasks[1].empty(),
    _receiveTasks[0].size(),
    _receiveTasks[1].size(),
    _currentReceiveBuffer
  );

  _readDeployBufferInReverseOrder                         = _wasTraversalInvertedThroughoutLastSendReceiveTraversal != isTraversalInverted;

  switchReceiveAndDeployBuffer(numberOfSentMessages);
}



template <class Data>
void peano::heap::BoundaryDataExchanger<Data>::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages(
  int numberOfMessagesSentThisIteration
) {
  logTraceInWith2Arguments( "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _rank, numberOfMessagesSentThisIteration );

  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;

  while (static_cast<int>(_receiveTasks[_currentReceiveBuffer].size()) < numberOfMessagesSentThisIteration ) {
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::BoundaryDataExchanger::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages",
         "waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages()", _rank,
         _tag, numberOfMessagesSentThisIteration - _receiveTasks[_currentReceiveBuffer].size()
       );

       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::BoundaryDataExchanger::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages",
         "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _rank,
         _tag, numberOfMessagesSentThisIteration - _receiveTasks[_currentReceiveBuffer].size()
       );
    }

    receiveDanglingMessages();
    tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }

  logTraceOutWith1Argument( "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _identifier );
}


template<class Data>
int peano::heap::BoundaryDataExchanger<Data>::releaseSentMessages() {
  const int result = static_cast<int>( _sendTasks.size() );
  for(typename std::vector<SendReceiveTask<Data> >::iterator i = _sendTasks.begin(); i != _sendTasks.end(); ++i) {
    MPI_Status status;

    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;
    int            finishedWait            = i->_metaInformation.getLength()==0 || i->_request;

    while (!finishedWait) {
      MPI_Test(&(i->_request), &finishedWait, &status);

      // deadlock aspect
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::SynchronousDataExchanger",
           "finishedToSendData()", i->_rank,-1,i->_metaInformation.getLength()
         );
         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
         std::ostringstream msg;
         msg << "tag " << _tag;
         tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::SynchronousDataExchanger",
           "finishedToSendData()", i->_rank,
           -1,
           i->_metaInformation.getLength(), msg.str()
         );
      }
      tarch::parallel::Node::getInstance().receiveDanglingMessages();
    }

    if (i->_metaInformation.getLength()>0) {
      delete[] (i->_data);
    }
  }

  _sendTasks.clear();
  return result;
}


template<class Data>
void peano::heap::BoundaryDataExchanger<Data>::finishedToSendData(bool isTraversalInverted) {
  #ifdef Asserts
  _isCurrentlySending = false;
  #endif

  _wasTraversalInvertedThroughoutLastSendReceiveTraversal = isTraversalInverted;
}


template<class Data>
void peano::heap::BoundaryDataExchanger<Data>::receiveDanglingMessages() {
  int        flag   = 0;
  MPI_Status status;
  int        result = MPI_Iprobe(
    _rank,
    _tag,
    tarch::parallel::Node::getInstance().getCommunicator(),
    &flag,
    &status
  );
  if (result!=MPI_SUCCESS) {
    logError(
      "receiveDanglingMessages()",
      "probing for messages failed: " << tarch::parallel::MPIReturnValueToString(result)
    );
  }
  if (flag) {
    logTraceInWith1Argument( "receiveDanglingMessages(...)", _tag );

    SendReceiveTask<Data> receiveTask;

    receiveTask._metaInformation.receive(_rank, _tag, true);
    receiveTask._rank = _rank;

    _numberOfReceivedMessages += 1;
    _numberOfSentRecords      += receiveTask._metaInformation.getLength();

    if(receiveTask._metaInformation.getLength() > 0) {
      receiveTask.triggerReceive(_tag);
      logDebug(
        "receiveDanglingMessages(...)",
        "started to receive " << _receiveTasks[_currentReceiveBuffer].size() << "th message from rank " << receiveTask._rank << " with " << receiveTask._metaInformation.getLength() << " entries and data pointing to " << receiveTask._data
      );
    #ifdef PackedEmptyHeapMessages
    } else if(receiveTask._metaInformation.getLength() < 0) {
      for(int i = 0; i < -receiveTask._metaInformation.getLength(); i++) {
        SendReceiveTask<Data> zeroLengthReceiveTask;
        zeroLengthReceiveTask._metaInformation = receiveTask._metaInformation;
        zeroLengthReceiveTask._rank = receiveTask._rank;
        zeroLengthReceiveTask._metaInformation.setLength(0);
        zeroLengthReceiveTask._data = 0;

        taskQueue.push_back(zeroLengthReceiveTask);
      }
    }
    #endif
    }
    _receiveTasks[_currentReceiveBuffer].push_back( receiveTask );

    logTraceOut( "receiveDanglingMessages(...)" );
  }
}


template<class Data>
void peano::heap::BoundaryDataExchanger<Data>::sendData(
  const std::vector<Data>&                      data,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  assertion4( _isCurrentlySending, _identifier, data.size(), position, level );

  SendReceiveTask<Data> sendTask;
  sendTask._rank = _rank;
  #ifdef Asserts
  //Set debug information
  sendTask._metaInformation.setPosition(position);
  sendTask._metaInformation.setLevel(level);
  #endif

  logDebug("sendData", "Sending data at " << position << " to Rank " << _rank << " with tag " << _tag  );

  sendTask._metaInformation.setLength(static_cast<int>( data.size() ));
  sendTask._metaInformation.send(_rank, _tag, true);

      #ifdef PackedEmptyHeapMessages
      assertion2(
        messageType != NeighbourCommunication ||
        data.size() > 0,
        data.size(), messageType
      );
      #endif

  if(data.size() > 0) {
    sendTask.wrapDataAndTriggerSend(data,_tag);
  }

  _sendTasks.push_back(sendTask);

  _numberOfSentMessages += 1;
  _numberOfSentRecords  += data.size();
}


template<class Data>
std::vector< Data > peano::heap::BoundaryDataExchanger<Data>::receiveData(
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  logTraceInWith4Arguments( "receiveData(...)", _identifier, _rank, position, level );

  const int currentDeployBuffer = 1-_currentReceiveBuffer;
  const int readElement         = _readDeployBufferInReverseOrder ? static_cast<int>(_receiveTasks[currentDeployBuffer].size())-1 : 0;
//  const int readElement         = _readDeployBufferInReverseOrder ? 0 : static_cast<int>(_receiveTasks[currentDeployBuffer].size())-1;
  assertion2( _receiveTasks[currentDeployBuffer].size()>0, tarch::parallel::Node::getInstance().getRank(), "if the neighbour data buffer is empty, you have perhaps forgotten to call releaseMessages() on the heap in the traversal before" );

  assertion6(
    _receiveTasks[currentDeployBuffer][readElement]._metaInformation.getLevel() == level,
    readElement,
    level,  position,
    _receiveTasks[currentDeployBuffer].front()._metaInformation.toString(),
    _receiveTasks[currentDeployBuffer].back()._metaInformation.toString(),
    tarch::parallel::Node::getInstance().getRank()
  );
  for (int d=0; d<DIMENSIONS; d++) {
    assertion7(
      tarch::la::equals(_receiveTasks[currentDeployBuffer][readElement]._metaInformation.getPosition(d), position(d)),
      readElement,
      level,  position,  d,
      _receiveTasks[currentDeployBuffer].front()._metaInformation.toString(),
      _receiveTasks[currentDeployBuffer].back()._metaInformation.toString(),
      tarch::parallel::Node::getInstance().getRank()
    );
  }

  const int numberOfElementsOfThisEntry = _receiveTasks[currentDeployBuffer][readElement]._metaInformation.getLength();
  assertion2(numberOfElementsOfThisEntry >= 0, position, level);
  assertion(_receiveTasks[currentDeployBuffer][readElement]._data!=0 || numberOfElementsOfThisEntry==0);

  const std::vector<Data> result = _receiveTasks[currentDeployBuffer][readElement].unwrapDataAndFreeMemory();
  if (readElement==0) {
    _receiveTasks[currentDeployBuffer].erase(_receiveTasks[currentDeployBuffer].begin());
  }
  else {
    _receiveTasks[currentDeployBuffer].pop_back();
  }
  logTraceOutWith1Argument( "receiveData(...)", result.size() );
  return result;
}


template<class Data>
void peano::heap::BoundaryDataExchanger<Data>::plotStatistics() const {
  logInfo(
    "plotStatistics()",
    "records sent by " << _identifier << " to " << _rank << " on tag " << _tag << ": " << _numberOfSentRecords
  );

  logInfo(
    "plotStatistics()",
    "messages sent by " << _identifier << " to " << _rank << " on tag " << _tag << ": " << _numberOfSentMessages
  );

  logInfo(
    "plotStatistics()",
    "records received by " << _identifier << " to " << _rank << " on tag " << _tag << ": " << _numberOfReceivedRecords
  );

  logInfo(
    "plotStatistics()",
    "messages received by " << _identifier << " to " << _rank << " on tag " << _tag << ": " << _numberOfReceivedMessages
  );

  logInfo(
    "plotStatistics()",
    "current send queue of " << _identifier << " holds " << _sendTasks.size() << " message(s)"
  );

  logInfo(
    "plotStatistics()",
    "current receive buffer of " << _identifier << " holds " << _receiveTasks[_currentReceiveBuffer].size() << " message(s)"
  );

  logInfo(
    "plotStatistics()",
    "current deploy buffer of " << _identifier << " holds " << _receiveTasks[1-_currentReceiveBuffer].size() << " message(s)"
  );
}


template<class Data>
void peano::heap::BoundaryDataExchanger<Data>::clearStatistics() {
  _numberOfSentMessages     = 0;
  _numberOfSentRecords      = 0;
  _numberOfReceivedMessages = 0;
  _numberOfReceivedRecords  = 0;
}


template <class Data>
void peano::heap::BoundaryDataExchanger<Data>::releaseReceivedNeighbourMessagesRequests() {
  bool allMessageCommunicationsAreFinished = false;

  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;
  int            finishedWait            = false;

  while (!allMessageCommunicationsAreFinished) {
    allMessageCommunicationsAreFinished = true;
    for(
      typename std::vector<SendReceiveTask<Data> >::iterator i = _receiveTasks[_currentReceiveBuffer].begin();
      i != _receiveTasks[_currentReceiveBuffer].end();
      i++
    ) {
      if(i->_metaInformation.getLength() > 0) {
        MPI_Status status;
        MPI_Test(&(i->_request), &finishedWait, &status);
        allMessageCommunicationsAreFinished &= (finishedWait!=0);
      }
    }

    // deadlock aspect
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::BoundaryDataExchanger",
         "releaseReceivedMessagesRequests()", -1,
         _tag, -1
       );
       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::BoundaryDataExchanger",
         "releaseReceivedMessagesRequests()", -1,
         _tag, -1
       );
    }

    tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }
}


template <class Data>
void peano::heap::BoundaryDataExchanger<Data>::switchReceiveAndDeployBuffer(int numberOfMessagesSentThisIteration) {
  assertion(_receiveTasks[1-_currentReceiveBuffer].empty());

  _currentReceiveBuffer = 1-_currentReceiveBuffer;

  const int sizeOfReceiveBuffer = static_cast<int>(_receiveTasks[1-_currentReceiveBuffer].size());
  assertion(sizeOfReceiveBuffer>=numberOfMessagesSentThisIteration);

  if (numberOfMessagesSentThisIteration < sizeOfReceiveBuffer) {
    // @todo raus
    logInfo(
      "switchReceiveAndDeployBuffer(map)",
      "have to copy back " << sizeOfReceiveBuffer-numberOfMessagesSentThisIteration <<
      " messages from the deploy buffer to the receive buffer"
    );

    typename std::vector<SendReceiveTask<Data> >::iterator p = _receiveTasks[1-_currentReceiveBuffer].begin();

    p += sizeOfReceiveBuffer;

    while (p != _receiveTasks[1-_currentReceiveBuffer].end()) {
      _receiveTasks[_currentReceiveBuffer].push_back(*p);
      p = _receiveTasks[1-_currentReceiveBuffer].erase(p);
    }
  }
  else {
    // @todo raus
    logInfo(
      "switchReceiveAndDeployBuffer(map)",
      "number of received messages equals number of sent messages"
    );
  }

  assertionEquals( static_cast<int>(_receiveTasks[1-_currentReceiveBuffer].size()), numberOfMessagesSentThisIteration);
}


#ifdef PackedEmptyHeapMessages
template <class Data, class MasterWorkerExchanger, class JoinForkExchanger, class NeighbourDataExchanger>
void peano::heap::Heap<Data, MasterWorkerExchanger, JoinForkExchanger, NeighbourDataExchanger>::compressZeroLengthMessage(
  int         toRank
) {
  if(_numberOfNeighbourDataCompressedEmptyMessages.find(toRank) == _numberOfNeighbourDataCompressedEmptyMessages.end()) {
    _numberOfNeighbourDataCompressedEmptyMessages[toRank] = 0;
  }
  _numberOfNeighbourDataCompressedEmptyMessages[toRank]++;
}

template <class Data, class MasterWorkerExchanger, class JoinForkExchanger, class NeighbourDataExchanger>
void peano::heap::Heap<Data, MasterWorkerExchanger, JoinForkExchanger, NeighbourDataExchanger>::sendCompressedEmptyMessages(
  int                                           toRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  logTraceInWith3Arguments("sendCompressedEmptyMessages", toRank, position, level);

  if(_numberOfNeighbourDataCompressedEmptyMessages.find(toRank) == _numberOfNeighbourDataCompressedEmptyMessages.end()) {
    _numberOfNeighbourDataCompressedEmptyMessages[toRank] = 0;
  }
  int numberOfCompressedMessages = _numberOfNeighbourDataCompressedEmptyMessages[toRank];
  _numberOfNeighbourDataCompressedEmptyMessages[toRank] = 0;

  if(numberOfCompressedMessages > 0) {
    int mpiTag = getTagForMessageType(NeighbourCommunication,true,0);
    SendReceiveTask<Data> sendTask;
    sendTask._rank = toRank;
    sendTask._metaInformation.setLength(static_cast<int>( -numberOfCompressedMessages ));
    #ifdef Asserts
    //Set debug information
    sendTask._metaInformation.setPosition(position);
    sendTask._metaInformation.setLevel(level);
    #endif
    sendTask._metaInformation.send(toRank, mpiTag, true);

    logDebug("sendData", "Sending data at " << position << " to Rank " << toRank << " with mpiTag: " << mpiTag << ", neighbour-data-tag: " << _mpiTagForNeighbourDataExchange << ", master/worker-tag=" << _mpiTagForMasterWorkerDataExchange << ", fork/join-tag=" << _mpiTagToExchangeForkJoinData);

    _neighbourDataSendTasks.push_back(sendTask);
  }

  logTraceOut("sendCompressedEmptyMessages");
}

template <class Data, class MasterWorkerExchanger, class JoinForkExchanger, class NeighbourDataExchanger>
void peano::heap::Heap<Data, MasterWorkerExchanger, JoinForkExchanger, NeighbourDataExchanger>::sendAllCompressedEmptyMessages() {
  logTraceIn("sendAllCompressedEmptyMessages");

  for(std::map<int, int>::iterator i = _numberOfNeighbourDataCompressedEmptyMessages.begin();
      i != _numberOfNeighbourDataCompressedEmptyMessages.end();
      i++) {
    sendCompressedEmptyMessages(i->first, 0, 0);
  }

  logTraceOut("sendAllCompressedEmptyMessages");
}
#endif
