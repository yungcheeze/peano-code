#include "tarch/Assertions.h"
#include "tarch/compiler/CompilerSpecificSettings.h"


template<class Data, class SendReceiveTaskType, class VectorContainer>
tarch::logging::Log  peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::_log( "peano::heap::BoundaryDataExchanger" );



template<class Data, class SendReceiveTaskType, class VectorContainer>
peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BoundaryDataExchanger():
  _identifier( "created-by-standard-constructor"),
  _metaDataTag(-1),
  _dataTag(-1),
  _rank(-1),
  _numberOfSentMessages(-1),
  _numberOfSentRecords(-1),
  _numberOfReceivedMessages(-1),
  _numberOfReceivedRecords(-1),
  _currentReceiveBuffer(-1),
  _readDeployBufferInReverseOrder(false),
  _wasTraversalInvertedThroughoutLastSendReceiveTraversal(false)
  #ifdef Asserts
  ,_isCurrentlySending(false)
  #endif
{
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BoundaryDataExchanger(
  const std::string& identifier,
  int metaDataTag,
  int dataTag,
  int rank
):
  _identifier(identifier),
  _metaDataTag(metaDataTag),
  _dataTag(dataTag),
  _rank(rank),
  _numberOfSentMessages(0),
  _numberOfSentRecords(0),
  _numberOfReceivedMessages(0),
  _numberOfReceivedRecords(0),
  _currentReceiveBuffer(0),
  _readDeployBufferInReverseOrder(false),
  _wasTraversalInvertedThroughoutLastSendReceiveTraversal(false)
  #ifdef Asserts
  ,_isCurrentlySending(false)
  #endif
  {
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::~BoundaryDataExchanger() {
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::startToSendData(bool isTraversalInverted) {
  logTraceInWith1Argument( "startToSendData(bool)", isTraversalInverted );

  assertion(_currentReceiveBuffer>=0);
  assertion(_currentReceiveBuffer<=1);
  assertionMsg(_receiveTasks[1-_currentReceiveBuffer].empty(), "finishedToSendData() has not been called before. Check whether allHeapsFinishedToSendBoundaryData() on AbstractHeap has been invoked or your mappings delegate heap management to kernel. If finish is called, this error pops up if we you don't pick up messages sent to a rank in the iteration before");

  assertion3(!_isCurrentlySending, _identifier, _rank, "call once per traversal" );

  #ifdef Asserts
  _isCurrentlySending = true;
  #endif

  const int numberOfSentMessages = getNumberOfSentMessages();

  releaseSentMessages();

  waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages(numberOfSentMessages);

  releaseReceivedNeighbourMessagesRequests();

  assertion3(
    _receiveTasks[0].empty() || _receiveTasks[1].empty(),
    _receiveTasks[0].size(),
    _receiveTasks[1].size(),
    _currentReceiveBuffer
  );

  _readDeployBufferInReverseOrder = _wasTraversalInvertedThroughoutLastSendReceiveTraversal != isTraversalInverted;

  switchReceiveAndDeployBuffer(numberOfSentMessages);
  postprocessStartToSendData();

  logTraceOut( "startToSendData(bool)" );
}



template <class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages(
  int numberOfMessagesSentThisIteration
) {
  logTraceInWith2Arguments( "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _rank, numberOfMessagesSentThisIteration );

  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;

  while (static_cast<int>(_receiveTasks[_currentReceiveBuffer].size()) < numberOfMessagesSentThisIteration ) {
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::BoundaryDataExchanger::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages",
         "waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages()", _rank,
         _metaDataTag, numberOfMessagesSentThisIteration - _receiveTasks[_currentReceiveBuffer].size()
       );

       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::BoundaryDataExchanger::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages",
         "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _rank,
         _metaDataTag, numberOfMessagesSentThisIteration - _receiveTasks[_currentReceiveBuffer].size()
       );
    }

    tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }

  logTraceOutWith2Arguments( "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _identifier, _receiveTasks[_currentReceiveBuffer].size() );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::releaseSentMessages() {
  logTraceInWith1Argument( "releaseSentMessages()", _sendTasks.size() );
  for(typename std::list<SendReceiveTaskType >::iterator i = _sendTasks.begin(); i != _sendTasks.end(); ++i) {
    MPI_Status status;

    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;
    int            finishedWait            = i->_metaInformation.getLength()==0;

    while (!finishedWait) {
      assertion1(i->_data!=nullptr, i->toString() );
      MPI_Test(&(i->_request), &finishedWait, &status);

      // deadlock aspect
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::SynchronousDataExchanger",
           "finishedToSendData()", i->_rank,-1,i->_metaInformation.getLength()
         );
         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
         std::ostringstream msg;
         msg << "metaDataTag=" << _metaDataTag;
         msg << ",dataTag=" << _dataTag;
         tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::SynchronousDataExchanger",
           "finishedToSendData()", i->_rank,
           -1,
           i->_metaInformation.getLength(), msg.str()
         );
      }
      tarch::parallel::Node::getInstance().receiveDanglingMessages();
    }

    i->freeMemory();
  }

  _sendTasks.clear();
  logTraceOutWith2Arguments( "releaseSentMessages()", _numberOfSentMessages, _numberOfSentRecords );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::finishedToSendData(bool isTraversalInverted) {
  logTraceInWith1Argument( "finishedToSendData(bool)", isTraversalInverted );

  assertion3(_isCurrentlySending, _identifier, _rank, "call once per traversal" );
  #ifdef Asserts
  _isCurrentlySending = false;
  #endif

  _wasTraversalInvertedThroughoutLastSendReceiveTraversal = isTraversalInverted;

  postprocessFinishedToSendData();
  logTraceOut( "finishedToSendData(bool)" );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::receiveDanglingMessages() {
  int        flag   = 1;
  MPI_Status status;
  
  while (flag) {
    int  result = MPI_Iprobe(
      _rank,
      _metaDataTag,
      tarch::parallel::Node::getInstance().getCommunicator(),
      &flag,
      &status
    );
    if (result!=MPI_SUCCESS) {
      logError(
        "receiveDanglingMessages()",
        "probing for messages failed: " << tarch::parallel::MPIReturnValueToString(result)
      );
    }
    if (flag) {
      logTraceInWith1Argument( "receiveDanglingMessages(...)", _metaDataTag );

      SendReceiveTaskType receiveTask;

      const int receiveMetaDataMode = -1;
      receiveTask._metaInformation.receive(_rank, _metaDataTag, true, receiveMetaDataMode);
      receiveTask._rank = _rank;

      _numberOfReceivedMessages += 1;
      _numberOfSentRecords      += receiveTask._metaInformation.getLength();

      handleAndQueueReceivedTask( receiveTask );

      logTraceOutWith1Argument( "receiveDanglingMessages(...)", receiveTask._metaInformation.toString() );
    }
  }
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::sendData(
  const Data* const                                    data,
  int                                                  count,
  const tarch::la::Vector<DIMENSIONS, double>&         position,
  int                                                  level
) {
  assertion5( _isCurrentlySending, _identifier, count, position, level, "forgot to call startToSendBoundaryData on heap?" );

  SendReceiveTaskType sendTask;
  sendTask._rank = _rank;
  #ifdef Asserts
  //Set debug information
  sendTask._metaInformation.setPosition(position);
  sendTask._metaInformation.setLevel(level);
  #endif

  logDebug("sendData", "sending data at " << position << " to Rank " << _rank << " with tag " << _dataTag  );

  sendTask._metaInformation.setLength(count);

  handleAndQueueSendTask( sendTask, data );

  _numberOfSentMessages += 1;
  _numberOfSentRecords  += count;
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
VectorContainer  peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::receiveData(
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  logTraceInWith4Arguments( "receiveData(...)", _identifier, _rank, position, level );

  assertion( _currentReceiveBuffer>=0 );
  assertion( _currentReceiveBuffer<=1 );

  const int currentDeployBuffer = 1-_currentReceiveBuffer;
  typename std::list<SendReceiveTaskType >::iterator readElement = _readDeployBufferInReverseOrder ? --_receiveTasks[currentDeployBuffer].end() : _receiveTasks[currentDeployBuffer].begin();

  assertion2( _receiveTasks[currentDeployBuffer].size()>0, tarch::parallel::Node::getInstance().getRank(), "if the neighbour data buffer is empty, you have perhaps forgotten to call releaseMessages() on the heap in the traversal before" );

  assertion10(
    readElement->fits(position,level) | !dataExchangerCommunicatesInBackground(),
    _readDeployBufferInReverseOrder,
    level,  position,
    _receiveTasks[currentDeployBuffer].front()._metaInformation.toString(),
    _receiveTasks[currentDeployBuffer].back()._metaInformation.toString(),
    tarch::parallel::Node::getInstance().getRank(),
    _rank,
    _identifier,
    dataExchangerCommunicatesInBackground(),
    "if _readDeployBufferInReverseOrder: take back() element"
  );

  #ifdef Asserts
  const int numberOfElementsOfThisEntry = readElement->_metaInformation.getLength();
  assertion2(numberOfElementsOfThisEntry >= 0, position, level);
  assertion(readElement->_data!=0 || numberOfElementsOfThisEntry==0);
  #endif

  const VectorContainer result(readElement->_data, readElement->_data+readElement->_metaInformation.getLength());

  readElement->freeMemory();
  _receiveTasks[currentDeployBuffer].erase(readElement);

  logTraceOutWith1Argument( "receiveData(...)", result.size() );
  return result;
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::plotStatistics() const {
  logInfo(
    "plotStatistics()",
    "records sent by " << _identifier << " to " << _rank << " on tag " << _metaDataTag << ": " << _numberOfSentRecords
  );

  logInfo(
    "plotStatistics()",
    "messages sent by " << _identifier << " to " << _rank << " on tag " << _metaDataTag << ": " << _numberOfSentMessages
  );

  logInfo(
    "plotStatistics()",
    "records received by " << _identifier << " to " << _rank << " on tag " << _metaDataTag << ": " << _numberOfReceivedRecords
  );

  logInfo(
    "plotStatistics()",
    "messages received by " << _identifier << " to " << _rank << " on tag " << _metaDataTag << ": " << _numberOfReceivedMessages
  );

  logInfo(
    "plotStatistics()",
    "current send queue of " << _identifier << " holds " << _sendTasks.size() << " message(s)"
  );

  logInfo(
    "plotStatistics()",
    "current receive buffer of " << _identifier << " holds " << _receiveTasks[_currentReceiveBuffer].size() << " message(s)"
  );

  logInfo(
    "plotStatistics()",
    "current deploy buffer of " << _identifier << " holds " << _receiveTasks[1-_currentReceiveBuffer].size() << " message(s)"
  );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::clearStatistics() {
  _numberOfSentMessages     = 0;
  _numberOfSentRecords      = 0;
  _numberOfReceivedMessages = 0;
  _numberOfReceivedRecords  = 0;
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::releaseReceivedNeighbourMessagesRequests() {
  logTraceIn( "releaseReceivedNeighbourMessagesRequests()" );
  bool allMessageCommunicationsAreFinished = !dataExchangerCommunicatesInBackground();

  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;
  int            finishedWait            = false;

  while (!allMessageCommunicationsAreFinished) {
    allMessageCommunicationsAreFinished = true;

    logDebug( "releaseReceivedNeighbourMessagesRequests()", "check all " << _receiveTasks[_currentReceiveBuffer].size() << " messages from rank " << _rank );

    for(
      typename std::list<SendReceiveTaskType >::iterator i = _receiveTasks[_currentReceiveBuffer].begin();
      i != _receiveTasks[_currentReceiveBuffer].end();
      i++
    ) {
      if(i->_metaInformation.getLength() > 0) {
        MPI_Status status;
        MPI_Test(&(i->_request), &finishedWait, &status);
        allMessageCommunicationsAreFinished &= (finishedWait!=0);
      }
    }

    // deadlock aspect
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::BoundaryDataExchanger",
         "releaseReceivedMessagesRequests()", -1,
         _metaDataTag, -1
       );
       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::BoundaryDataExchanger",
         "releaseReceivedMessagesRequests()", -1,
         _metaDataTag, -1
       );
    }

    // @see Remark on overtaking messages in switchReceiveAndDeployBuffer()
    //      It explains that this line has to be commented out.
    // tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }
  logTraceOut( "releaseReceivedNeighbourMessagesRequests()" );
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::switchReceiveAndDeployBuffer(int numberOfMessagesSentThisIteration) {
  assertion(_receiveTasks[1-_currentReceiveBuffer].empty());

  _currentReceiveBuffer = 1-_currentReceiveBuffer;

  const int sizeOfNewDeployBuffer = static_cast<int>(_receiveTasks[1-_currentReceiveBuffer].size());
  assertion(sizeOfNewDeployBuffer>=numberOfMessagesSentThisIteration);

  if (numberOfMessagesSentThisIteration < sizeOfNewDeployBuffer) {
    logDebug(
      "switchReceiveAndDeployBuffer(int)",
      "have to copy back " << sizeOfNewDeployBuffer-numberOfMessagesSentThisIteration <<
      " message(s) from the deploy buffer to the receive buffer or rank " << _rank <<
      ", as " << numberOfMessagesSentThisIteration <<
      " message(s) have been sent out while " << sizeOfNewDeployBuffer << " have been received"
    );

    typename std::list<SendReceiveTaskType >::iterator p = _receiveTasks[1-_currentReceiveBuffer].begin();
    std::advance(p, numberOfMessagesSentThisIteration);

    _receiveTasks[_currentReceiveBuffer].insert(
      _receiveTasks[_currentReceiveBuffer].begin(),
      p,
      _receiveTasks[1-_currentReceiveBuffer].end()
    );
    _receiveTasks[1-_currentReceiveBuffer].erase(
      p,
      _receiveTasks[1-_currentReceiveBuffer].end()
    );

    for (
      typename std::list<SendReceiveTaskType >::const_iterator it = _receiveTasks[_currentReceiveBuffer].begin();
      it != _receiveTasks[_currentReceiveBuffer].end();
      it++
    ) {
      logDebug( "switchReceiveAndDeployBuffer(int)", "transferred element (now in receive buffer): " << it->toString() );
    }

    for (
      typename std::list<SendReceiveTaskType >::const_iterator it = _receiveTasks[1-_currentReceiveBuffer].begin();
      it != _receiveTasks[1-_currentReceiveBuffer].end();
      it++
    ) {
      logDebug( "switchReceiveAndDeployBuffer(int)", "remaining deploy element: " << it->toString() );
    }
  }
  else {
    logDebug(
      "switchReceiveAndDeployBuffer(int)",
      "number of received messages from rank " << _rank << " equals number of sent messages: " << numberOfMessagesSentThisIteration
    );
  }

  assertionEquals( static_cast<int>(_receiveTasks[1-_currentReceiveBuffer].size()), numberOfMessagesSentThisIteration);
}
