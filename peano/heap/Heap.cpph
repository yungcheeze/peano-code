#include <limits>
#include <memory.h> 

#ifdef Parallel
#include <mpi.h>
#endif

#include "tarch/Assertions.h"
#include "tarch/services/ServiceRepository.h"


#include "tarch/services/ServiceFactory.h"


template <class Data>
tarch::logging::Log peano::heap::Heap<Data>::_log("peano::heap::Heap");


template <class Data>
peano::heap::Heap<Data>::Heap()
  : _heapData(), _freedHeapIndices(), _nextIndex(0)
  #ifdef Parallel
    , _mpiTagForNeighbourDataExchange(tarch::parallel::Node::reserveFreeTag("heap[neighbour]"))
    , _mpiTagForMasterWorkerDataExchange(tarch::parallel::Node::reserveFreeTag("heap[master-worker]"))
    , _mpiTagToExchangeForkJoinData(tarch::parallel::Node::reserveFreeTag("heap[join/fork]"))
  #endif
  , _neighbourDataSendTasks()
  ,_masterWorkerDataSendTasks()
  ,_forkJoinDataSendTasks()
  ,_masterWorkerDataReceiveTasks()
  ,_forkJoinDataReceiveTasks()
  ,_numberOfRecordsSentToNeighbour(0)
  ,_numberOfRecordsSentToMasterWorker(0)
  ,_numberOfRecordsSentDueToForkJoin(0)
  ,_numberOfMessagesSentToNeighbour(0)
  ,_numberOfMessagesSentToMasterWorker(0)
  ,_numberOfMessagesSentDueToForkJoin(0)
  ,_numberOfMessagesReceivedFromNeighbour(0)
  ,_numberOfMessagesReceivedFromMasterWorker(0)
  ,_numberOfMessagesReceivedDueToForkJoin(0)
  ,_currentReceiveBuffer(0)
  ,_maximumNumberOfHeapEntries(0)
  ,_numberOfHeapAllocations(0)
  ,_numberOfHeapFrees(0)
  ,_name("<heap name not set>")
  ,_readDeployBufferInReverseOrder(true)
  ,_wasTraversalInvertedThroughoutLastSendReceiveTraversal(false)
  ,_heapIsCurrentlySentReceived(false)
{
  #ifdef Parallel
  if (SendReceiveTask::MPIData::Datatype==0) {
    SendReceiveTask::MPIData::initDatatype();
  }
  if(MetaInformation::Datatype==0) {
    MetaInformation::initDatatype();
  }
  #endif

  tarch::services::ServiceRepository::getInstance().addService( this, "peano::heap::Heap<Data>" );
}


template <class Data>
peano::heap::Heap<Data>::~Heap()
{
  deleteAllData();
}


template <class Data>
void peano::heap::Heap<Data>::deleteAllData() {
  for(typename std::map<int, std::vector<Data>*>::iterator i = _heapData.begin(); i != _heapData.end(); i++) {
    assertionMsg((*i).second != 0, _name << ": Null-pointer was stored in heap data map.");
    delete (*i).second;
  }
  _heapData.clear();
}


template <class Data>
void peano::heap::Heap<Data>::plotStatistics() {
  if(_name != "") {
    logInfo("plotStatistics()", "Statistics for " << _name);
  }
  logInfo("plotStatistics()", "size of heap: " << _heapData.size() << " entries" );
  logInfo("plotStatistics()", "freed but not reassigned heap indices: " << _freedHeapIndices.size() );

  logInfo("plotStatistics()", "maximum number of allocated heap entries: " << _maximumNumberOfHeapEntries );
  logInfo("plotStatistics()", "number of heap allocations: " << _numberOfHeapAllocations );
  logInfo("plotStatistics()", "number of heap frees: " << _numberOfHeapFrees );

  logInfo("plotStatistics()", "records sent to neighbours: " << _numberOfRecordsSentToNeighbour << " (" << (_numberOfRecordsSentToNeighbour * sizeof(Data)) <<  " Byte(s)) in " << _numberOfMessagesSentToNeighbour << " message(s)" );
  logInfo("plotStatistics()", "records sent to master and workers: " << _numberOfRecordsSentToMasterWorker << " (" << (_numberOfRecordsSentToMasterWorker * sizeof(Data)) <<  " Byte(s)) in " << _numberOfMessagesSentToMasterWorker << " message(s)" );
  logInfo("plotStatistics()", "records sent due to join or fork: " << _numberOfRecordsSentDueToForkJoin << " (" << (_numberOfRecordsSentDueToForkJoin * sizeof(Data)) <<  " Byte(s)) in " << _numberOfMessagesSentDueToForkJoin << " message(s)" );

  logInfo("plotStatistics()", "messages received from neighbours: "         << _numberOfMessagesReceivedFromNeighbour  );
  logInfo("plotStatistics()", "messages received from master and workers: " << _numberOfMessagesReceivedFromMasterWorker  );
  logInfo("plotStatistics()", "messages received due to join or fork: "     << _numberOfMessagesReceivedDueToForkJoin  );
}


template <class Data>
void peano::heap::Heap<Data>::clearStatistics() {
  _maximumNumberOfHeapEntries   = 0;
  _numberOfHeapAllocations      = 0;
  _numberOfHeapFrees            = 0;

  _numberOfRecordsSentToNeighbour    = 0;
  _numberOfRecordsSentToMasterWorker = 0;
  _numberOfRecordsSentDueToForkJoin  = 0;

  _numberOfMessagesSentToNeighbour    = 0;
  _numberOfMessagesSentToMasterWorker = 0;
  _numberOfMessagesSentDueToForkJoin  = 0;

  _numberOfMessagesReceivedFromNeighbour    = 0;
  _numberOfMessagesReceivedFromMasterWorker = 0;
  _numberOfMessagesReceivedDueToForkJoin    = 0;
}


template <class Data>
peano::heap::Heap<Data>& peano::heap::Heap<Data>::getInstance() {
  static peano::heap::Heap<Data> instance;
  return instance;
}


template <class Data>
int peano::heap::Heap<Data>::createData(int numberOfEntries) {
  logTraceInWith1Argument( "createData()", numberOfEntries );

  int index;
  if (_freedHeapIndices.empty()) {
    index = _nextIndex;
    _nextIndex++;
  }
  else {
    index = _freedHeapIndices.front();
    _freedHeapIndices.pop_front();
  }

  assertionMsg(_heapData.find(index) == _heapData.end(), "heap entry does exist already.");
  _heapData.insert( typename HeapContainer::value_type(index,new std::vector<Data>(numberOfEntries)) );

  assertionMsg(_heapData.find(index) != _heapData.end(), "insertion of heap data not successful.");
  assertion(index >= 0);

  _numberOfHeapAllocations += 1.0;

  if(static_cast<int>(_heapData.size()) > _maximumNumberOfHeapEntries) {
    _maximumNumberOfHeapEntries = static_cast<int>(_heapData.size());
  }

  logTraceOutWith2Arguments("createData()", index, _numberOfHeapAllocations);
  return index;
}


template <class Data>
std::vector<Data>& peano::heap::Heap<Data>::getData(int index) {
  #ifdef Asserts
  std::string message = "Trying to get heap data for unknown index. Has the index been initialized correctly?";
  #endif
  assertion4(_heapData.find(index) != _heapData.end(), _name, message, index, _heapData.size());
  assertionMsg(_heapData[index] != 0, _name << ": Null-pointer was stored in heap data map");
  return *(_heapData[index]);
}


template <class Data>
const std::vector<Data>& peano::heap::Heap<Data>::getData(int index) const {
  #ifdef Asserts
  std::string message = "Trying to get heap data for unknown index. Has the index been initialized correctly?";
  #endif
  assertion4(_heapData.find(index) != _heapData.end(), _name, message, index, _heapData.size());
  assertionMsg(_heapData[index] != 0, _name << ": Null-pointer was stored in heap data map");
  return *(_heapData[index]);
}


template <class Data>
void peano::heap::Heap<Data>::moveData( int toIndex, int fromIndex ) {
  assertion4(_heapData.find(toIndex) != _heapData.end(), _name, toIndex, fromIndex, _heapData.size());
  assertion4(_heapData.find(fromIndex) != _heapData.end(), _name, toIndex, fromIndex, _heapData.size());

  _heapData[toIndex]->insert( _heapData[toIndex]->end(), _heapData[fromIndex]->begin(), _heapData[fromIndex]->end() );
  _heapData[fromIndex]->clear();
}


template <class Data>
bool peano::heap::Heap<Data>::isValidIndex(int index) const {
    return _heapData.find(index) != _heapData.end();
}


template <class Data>
void peano::heap::Heap<Data>::deleteData(int index) {
  logTraceInWith2Arguments("deleteData(int)", _name, index);

  #ifdef Asserts
  std::string message = "Trying to get heap data for unknown index. Has the index been initialized correctly?";
  #endif
  assertion4(_heapData.find(index) != _heapData.end(), _name, message, index, _heapData.size());
  assertionMsg(_heapData[index] != 0, _name << ": Null-pointer was stored in heap data map");

  _heapData[index]->clear();
  delete _heapData[index];
  _heapData.erase(index);

  _freedHeapIndices.push_back(index);
  _numberOfHeapFrees++;

  logTraceOut("deleteData(int)");
}


template <class Data>
int peano::heap::Heap<Data>::getNumberOfAllocatedEntries() const {
  return _heapData.size();
}


template <class Data>
void peano::heap::Heap<Data>::restart() {
  deleteAllData();
}


template <class Data>
void peano::heap::Heap<Data>::shutdown() {
  deleteAllData();
}


template <class Data>
void peano::heap::Heap<Data>::setName(std::string name) {
  _name = name;
}


template <class Data>
void peano::heap::Heap<Data>::sendData(
  const std::vector< Data >&                    data,
  int                                           toRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  logTraceInWith6Arguments( "sendData(...)", _name, toRank, position, level, messageType, data.size() );

  #ifdef Parallel
  int mpiTag = getTagForMessageType(messageType,true,data.size());

  SendReceiveTask sendTask;

  sendTask._rank = toRank;

  #ifdef Asserts
  //Set debug information
  sendTask._metaInformation.setPosition(position);
  sendTask._metaInformation.setLevel(level);
  #endif

  logDebug("sendData", "Sending data at " << position << " to Rank " << toRank << " with mpiTag: " << mpiTag << ", neighbour-data-tag: " << _mpiTagForNeighbourDataExchange << ", master/worker-tag=" << _mpiTagForMasterWorkerDataExchange << ", fork/join-tag=" << _mpiTagToExchangeForkJoinData);

  sendTask._metaInformation.setLength(static_cast<int>( data.size() ));
  sendTask._metaInformation.send(toRank, mpiTag, true);

  assertion((messageType==ForkOrJoinCommunication) || (messageType==MasterWorkerCommunication) || _heapIsCurrentlySentReceived);

  if(data.size() > 0) {
    sendTask._data = new typename SendReceiveTask::MPIData[data.size()];
    for (int i=0; i<static_cast<int>( data.size() ); i++) {
      #if defined(ParallelExchangePackedRecordsInHeaps)
      sendTask._data[i] = data[i].convert();
      #else
      sendTask._data[i] = data[i];
      #endif
    }
    int result = MPI_Isend(
      sendTask._data, sendTask._metaInformation.getLength(), SendReceiveTask::MPIData::Datatype, toRank,
      mpiTag,
      tarch::parallel::Node::getInstance().getCommunicator(), &sendTask._request
    );

    if ( result != MPI_SUCCESS ) {
      logError(
        "sendData(...)", "failed to send heap data to node "
        << toRank << ": " << tarch::parallel::MPIReturnValueToString(result)
      );
    }
  }

  if (messageType == NeighbourCommunication) {
    _neighbourDataSendTasks.push_back(sendTask);
  }
  else if (messageType == MasterWorkerCommunication) {
    _masterWorkerDataSendTasks.push_back(sendTask);
  } else {
    _forkJoinDataSendTasks.push_back(sendTask);
  }
  #endif

  logTraceOut( "sendData(...)" );
}


template <class Data>
void peano::heap::Heap<Data>::sendData(
  int                                           index,
  int                                           toRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  logTraceInWith6Arguments( "sendData(...)", _name, index, toRank, position, level, messageType );

  sendData( getData(index), toRank, position, level, messageType );

  logTraceOutWith1Argument( "sendData(...)", getData(index).size() );
}


template <class Data>
void peano::heap::Heap<Data>::releaseSentMessages(std::vector<SendReceiveTask>& tasks) {
  #ifdef Parallel
  logTraceInWith1Argument( "releaseSentMessages()", tasks.size() );

  for(typename std::vector<SendReceiveTask>::iterator i = tasks.begin(); i != tasks.end(); ++i) {
    MPI_Status status;

    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;
    int            finishedWait            = i->_metaInformation.getLength()==0 || i->_request;

    while (!finishedWait) {
      MPI_Test(&(i->_request), &finishedWait, &status);

      // deadlock aspect
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::Heap",
           "releaseSentMessages()", i->_rank,-1,i->_metaInformation.getLength()
         );
         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
         std::ostringstream msg;
         msg << "tag either " << _mpiTagForNeighbourDataExchange
             << ", " << _mpiTagForMasterWorkerDataExchange
             << " or " << _mpiTagToExchangeForkJoinData;
         tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::Heap",
           "releaseSentMessages()", i->_rank,
           -1,
           i->_metaInformation.getLength(), msg.str()
         );
      }
      tarch::parallel::Node::getInstance().receiveDanglingMessages();
    }

    if (i->_metaInformation.getLength()>0) {
      delete[] (i->_data);
    }
  }
  logTraceOut( "releaseSentMessages()" );
  #endif
}


template <class Data>
std::map<int,int> peano::heap::Heap<Data>::releaseSentNeighbourMessages() {
  std::map<int,int> result;

  #ifdef Parallel
  logTraceInWith3Arguments( "releaseSentNeighbourMessages()", _neighbourDataSendTasks.size(), _masterWorkerDataSendTasks.size(), _forkJoinDataSendTasks.size() );

  for (typename std::vector<SendReceiveTask>::const_iterator p=_neighbourDataSendTasks.begin(); p!=_neighbourDataSendTasks.end(); p++) {
    if (result.count(p->_rank)==0) {
      result.insert( std::pair<int,int>(p->_rank,0));
    }
    result[ p->_rank ] ++;
  }

  releaseSentMessages( _neighbourDataSendTasks );

  logTraceOut( "releaseSentNeighbourMessages()" );
  #endif

  return result;
}


template <class Data>
void peano::heap::Heap<Data>::releaseAndClearSentForkJoinAndMasterWorkerMessages() {
  #ifdef Parallel
  logTraceInWith3Arguments( "releaseAndClearSentForkJoinAndMasterWorkerMessages()", _neighbourDataSendTasks.size(), _masterWorkerDataSendTasks.size(), _forkJoinDataSendTasks.size() );

  releaseSentMessages( _masterWorkerDataSendTasks );
  releaseSentMessages( _forkJoinDataSendTasks );

  _masterWorkerDataSendTasks.clear();
  _forkJoinDataSendTasks.clear();

  logTraceOut( "releaseAndClearSentForkJoinAndMasterWorkerMessages()" );
  #endif
}



template <class Data>
void peano::heap::Heap<Data>::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages(const std::map<int,int>&  statistics) {
  logTraceInWith2Arguments( "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _name, getSizeOfSendBuffer() );

  #ifdef Parallel
  for (
    std::map<int,int>::const_iterator p = statistics.begin();
    p != statistics.end();
    p++) {
    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;
    while (getSizeOfReceiveBuffer(p->first)<p->second) {
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::Heap::releaseMessages",
           "waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages()", -1,
           _mpiTagForNeighbourDataExchange, -1
         );

         logDebug("waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages", "send=" << p->first << ", receive=" << p->second );

         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
         tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::Heap::releaseMessages",
           "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", -1,
           _mpiTagForNeighbourDataExchange, -1
         );
      }

      tarch::parallel::Node::getInstance().receiveDanglingMessages();
    }

  }
  #endif

  logTraceOutWith2Arguments( "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _name, getSizeOfSendBuffer() );
}


template <class Data>
void peano::heap::Heap<Data>::releaseReceivedNeighbourMessagesRequests() {
  #ifdef Parallel
  // I originally used a plain for loop over the receive buffer, but that does not
  // work, as a receiveDanglingMessages within this for loop might change the
  //  underlying  data container.
  bool allMessageCommunicationsAreFinished = false;

  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;
  int            finishedWait            = false;

  while (!allMessageCommunicationsAreFinished) {
    allMessageCommunicationsAreFinished = true;
    for(typename std::vector<SendReceiveTask>::iterator i = _neighbourDataReceiveTasks[_currentReceiveBuffer].begin(); i != _neighbourDataReceiveTasks[_currentReceiveBuffer].end(); ++i) {
      if(i->_metaInformation.getLength() > 0) {
        MPI_Status status;
        MPI_Test(&(i->_request), &finishedWait, &status);
        allMessageCommunicationsAreFinished &= (finishedWait!=0);
      }
    }

    // deadlock aspect
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::Heap",
         "releaseReceivedMessagesRequests()", -1,
         _mpiTagForNeighbourDataExchange, -1
       );
       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::Heap",
         "releaseReceivedMessagesRequests()", -1,
         _mpiTagForNeighbourDataExchange, -1
       );
    }

    tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }
  #endif
}


template <class Data>
void peano::heap::Heap<Data>::removeMessageFromBuffer(int number, std::vector<SendReceiveTask>& tasks) {
  assertion( number>=0 );
  assertion( number<static_cast<int>(tasks.size()) );

  typename std::vector<SendReceiveTask>::iterator it = tasks.begin();
  it+=number;

  if (it->_metaInformation.getLength()>0) {
    assertion(it->_data != 0);
    delete[] it->_data;
  }
  else {
    assertion(it->_data == 0);
  }
  tasks.erase(it);
}


template <class Data>
int peano::heap::Heap<Data>::findMessageFromRankInNeighbourDataDeployBuffer(int ofRank) const {
  const int currentDeployBuffer = 1-_currentReceiveBuffer;
  if (_readDeployBufferInReverseOrder) {
    for (int i=getSizeOfDeployBuffer()-1; i>=0; i--) {
      if (_neighbourDataReceiveTasks[currentDeployBuffer][i]._rank==ofRank) {
        return i;
      }
    }
  }
  else {
    for (int i=0; i<getSizeOfDeployBuffer(); i++) {
      if (_neighbourDataReceiveTasks[currentDeployBuffer][i]._rank==ofRank) {
        return i;
      }
    }
  }
  assertionMsg( false, "no fitting message found in neighbour data buffer" );
  return -1;
}


template <class Data>
int peano::heap::Heap<Data>::findMessageFromRankInMasterWorkerOrForkJoinDataBuffer(
  int                           ofRank,
  std::vector<SendReceiveTask>& tasks
) {
  #ifdef Parallel
  for (int i=0; i<static_cast<int>(tasks.size()); i++) {
    if (tasks[i]._rank==ofRank) {
      MPI_Status status;
      int        finishedWait = false;
      if (tasks[i]._metaInformation.getLength() > 0) {
        MPI_Test(&(tasks[i]._request), &finishedWait, &status);
      }
      else {
        finishedWait = true;
      }

      if (finishedWait) {
        return i;
      }
      else {
        return -1;
      }
    }
  }
  #endif
  return -1;
}

template <class Data>
std::vector< Data > peano::heap::Heap<Data>::receiveMasterWorkerOrForkJoinData(
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  logTraceInWith1Argument( "receiveMasterWorkerOrForkJoinData(...)", fromRank );

  assertion(messageType == MasterWorkerCommunication || messageType == ForkOrJoinCommunication);
  assertion(messageType!=NeighbourCommunication);

  std::vector< Data >  result;
  #ifdef Parallel
  const int                     mpiTag       = getTagForMessageType(messageType,false,0);
  std::vector<SendReceiveTask>& receiveTasks = (messageType == MasterWorkerCommunication) ? _masterWorkerDataReceiveTasks : _forkJoinDataReceiveTasks;
  int                           resultTask   = findMessageFromRankInMasterWorkerOrForkJoinDataBuffer(fromRank, receiveTasks);

  logDebug("receiveMasterWorkerOrForkJoinData", "Receiving data synchronously with mpiTag: " << mpiTag << ", fork/join-tag=" << _mpiTagToExchangeForkJoinData << ", neighbour-data-tag=" << _mpiTagForNeighbourDataExchange << ", master/worker-tag=" <<  _mpiTagForMasterWorkerDataExchange);

  if (resultTask == -1) {
    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;

    while (resultTask == -1) {
      resultTask = findMessageFromRankInMasterWorkerOrForkJoinDataBuffer(fromRank, receiveTasks);

      // deadlock aspect
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::Heap",
           "receiveMasterWorkerOrForkJoinData()", fromRank,
           mpiTag, -1
         );
         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
         tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::Heap",
           "receiveMasterWorkerOrForkJoinData()", fromRank,
           mpiTag, -1
         );
      }
      tarch::parallel::Node::getInstance().receiveDanglingMessages();
    }
  }

  result = extractMessageFromTaskQueue(resultTask, receiveTasks, position, level);
  removeMessageFromBuffer( resultTask, receiveTasks );
  #endif

  logTraceOutWith1Argument( "receiveMasterWorkerOrForkJoinData(...)", result.size() );
  return result;
}

template <class Data>
std::vector< Data > peano::heap::Heap<Data>::extractMessageFromTaskQueue(
  int                                           messageNumber,
  std::vector<SendReceiveTask>&                 tasks,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  std::vector< Data > result;

  #ifdef Parallel

  const int numberOfElementsOfThisEntry = tasks[messageNumber]._metaInformation.getLength();

  assertionEquals4(
    tasks[messageNumber]._metaInformation.getLevel(),
    level,
    tasks[messageNumber]._metaInformation.toString(),
    level,  position,
    tarch::parallel::Node::getInstance().getRank()
  );
  for (int d=0; d<DIMENSIONS; d++) {
    assertionNumericalEquals5(
      tasks[messageNumber]._metaInformation.getPosition(d),
      position(d),
      tasks[messageNumber]._metaInformation.toString(),
      level,  position,  d,
      tarch::parallel::Node::getInstance().getRank()
    );
  }

  assertion(tasks[messageNumber]._data!=0 || numberOfElementsOfThisEntry==0);

  for (int i=0; i<numberOfElementsOfThisEntry; i++) {
    #if defined(ParallelExchangePackedRecordsInHeaps)
    result.push_back(tasks[messageNumber]._data[i].convert());
    #else
    result.push_back(tasks[messageNumber]._data[i]);
    #endif
  }

  #endif

  return result;
}


template <class Data>
std::vector< Data > peano::heap::Heap<Data>::receiveNeighbourData(
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  logTraceInWith6Arguments( "receiveNeighbourData(...)", fromRank, position, level, _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );

  std::vector< Data > result;

  #ifdef Parallel
  assertion1(_heapIsCurrentlySentReceived, tarch::parallel::Node::getInstance().getRank());
  assertion3( getSizeOfDeployBuffer()>0, tarch::parallel::Node::getInstance().getRank(), toString(), "if the neighbour data buffer is empty, you have perhaps forgotten to call releaseMessages() on the heap in the traversal before" );

  const int currentDeployBuffer         = 1-_currentReceiveBuffer;
  const int elementFromDeployBuffer     = findMessageFromRankInNeighbourDataDeployBuffer(fromRank);

  result = extractMessageFromTaskQueue(elementFromDeployBuffer,_neighbourDataReceiveTasks[currentDeployBuffer],position,level);
  removeMessageFromBuffer( elementFromDeployBuffer, _neighbourDataReceiveTasks[currentDeployBuffer] );
  #endif

  logTraceOutWith1Argument( "receiveNeighbourData(...)", result.size() );
  return result;
}


template <class Data>
std::vector< Data > peano::heap::Heap<Data>::receiveData(
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  if (messageType == NeighbourCommunication) {
    return receiveNeighbourData(fromRank, position, level);
  } else {
    return receiveMasterWorkerOrForkJoinData(fromRank, position, level, messageType);
  }
}



template <class Data>
int peano::heap::Heap<Data>::receiveData(
  int                                           index,
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  std::vector< Data >   receivedData;
  std::vector< Data >&  localData = getData(index);
  if (messageType == NeighbourCommunication) {
    receivedData = receiveNeighbourData(fromRank, position, level);
  } else {
    receivedData = receiveMasterWorkerOrForkJoinData(fromRank, position, level, messageType);
  }
  localData.insert( localData.end(), receivedData.begin(), receivedData.end() );
  return static_cast<int>( receivedData.size() );
}


template <class Data>
void peano::heap::Heap<Data>::receiveDanglingMessages(int tag, std::vector<SendReceiveTask>& taskQueue) {
  #ifdef Parallel
  int        flag   = 0;
  MPI_Status status;
  int        result = MPI_Iprobe(
    MPI_ANY_SOURCE,
    tag,
    tarch::parallel::Node::getInstance().getCommunicator(),
    &flag,
    &status
  );
  if (result!=MPI_SUCCESS) {
    logError(
      "receiveDanglingMessages()",
      "probing for messages failed: " << tarch::parallel::MPIReturnValueToString(result)
    );
  }
  if (flag) {
    logTraceInWith1Argument( "receiveDanglingMessages(...)", tag );

    SendReceiveTask receiveTask;

    receiveTask._metaInformation.receive(status.MPI_SOURCE, tag, true);
    receiveTask._rank = status.MPI_SOURCE;

    if(receiveTask._metaInformation.getLength() > 0) {
      receiveTask._data = new typename SendReceiveTask::MPIData[ receiveTask._metaInformation.getLength() ];

      result = MPI_Irecv(receiveTask._data, receiveTask._metaInformation.getLength(), SendReceiveTask::MPIData::Datatype, status.MPI_SOURCE, tag, tarch::parallel::Node::getInstance().getCommunicator(), &receiveTask._request);
      if ( result != MPI_SUCCESS ) {
        logError(
          "receiveDanglingMessages()",
          "failed to receive heap data from node "
          << status.MPI_SOURCE << ": " << tarch::parallel::MPIReturnValueToString(result)
        );
      }
    }

    taskQueue.push_back( receiveTask );

    logDebug(
      "receiveDanglingMessages(...)",
      "started to receive " << taskQueue.size() << "th message from rank " << receiveTask._rank << " with " << receiveTask._metaInformation.getLength() << " entries and data pointing to " << receiveTask._data
    );

    logTraceOutWith3Arguments( "receiveDanglingMessages(...)", _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );
  }
  #endif
}


template <class Data>
void peano::heap::Heap<Data>::receiveDanglingMessages() {
  #ifdef Parallel
  receiveDanglingMessages(_mpiTagToExchangeForkJoinData , _forkJoinDataReceiveTasks);
  receiveDanglingMessages(_mpiTagForNeighbourDataExchange, _neighbourDataReceiveTasks[_currentReceiveBuffer]);
  receiveDanglingMessages(_mpiTagForMasterWorkerDataExchange, _masterWorkerDataReceiveTasks);
  #endif
}

template <class Data>
int peano::heap::Heap<Data>::getTagForMessageType(MessageType messageType, bool isSendTask, int messageSize) {
  #ifdef Parallel
    switch(messageType) {
      case NeighbourCommunication:
        if (isSendTask) {
          _numberOfRecordsSentToNeighbour  += messageSize;
          _numberOfMessagesSentToNeighbour += 1;
        }
        else {
          _numberOfMessagesReceivedFromNeighbour += 1;
        }
        return _mpiTagForNeighbourDataExchange;
      case MasterWorkerCommunication:
        if (isSendTask) {
          _numberOfRecordsSentToMasterWorker  += messageSize;
          _numberOfMessagesSentToMasterWorker += 1;
        }
        else {
          _numberOfMessagesReceivedFromMasterWorker += 1;
        }
        return _mpiTagForMasterWorkerDataExchange;
      case ForkOrJoinCommunication:
        if (isSendTask) {
          _numberOfRecordsSentDueToForkJoin  += messageSize;
          _numberOfMessagesSentDueToForkJoin += 1;
        }
        else {
          _numberOfMessagesReceivedDueToForkJoin += 1;
        }
        return _mpiTagToExchangeForkJoinData;
      default:
        assertionMsg( false, "not supported" );
        return -1;
    }
  #else
  return -1;
  #endif
}

template <class Data>
int peano::heap::Heap<Data>::getSizeOfReceiveBuffer(int neighbourRank) const {
  int result = 0;
  for (
    typename std::vector<SendReceiveTask>::const_iterator p = _neighbourDataReceiveTasks[_currentReceiveBuffer].begin();
    p != _neighbourDataReceiveTasks[_currentReceiveBuffer].end();
    p++
  ) {
    if (p->_rank==neighbourRank) {
      result ++;
    }
  }
  return result;
}


template <class Data>
int peano::heap::Heap<Data>::getSizeOfDeployBuffer() const {
  return static_cast<int>(_neighbourDataReceiveTasks[1-_currentReceiveBuffer].size());
}


template <class Data>
int peano::heap::Heap<Data>::getSizeOfSendBuffer() const {
  return static_cast<int>(_neighbourDataSendTasks.size());
}


template <class Data>
std::string peano::heap::Heap<Data>::toString() const {
  std::ostringstream msg;

  msg << "(name=" << _name
      << ",heap-map-size=" << _heapData.size();

  #ifdef Parallel
  msg << ",mpi-tag-neighbour-data=" << _mpiTagForNeighbourDataExchange
      << ",mpit-tag-master-worker-data=" << _mpiTagForMasterWorkerDataExchange
      << ",mpi-tag-join/fork-data=" << _mpiTagToExchangeForkJoinData
      << ",read-deploy-buffer-in-reverse-order=" << _readDeployBufferInReverseOrder
      << ",current-receive-buffer=" << _currentReceiveBuffer
      << ",unfinished-send-tasks=" << _neighbourDataSendTasks.size()
      << ",neighbour-data-receive-buffer-size-0=" << _neighbourDataReceiveTasks[0].size()
      << ",neighbour-data-receive-buffer-size-1=" << _neighbourDataReceiveTasks[1].size();
  #endif

  msg << ")";

  return msg.str();
}


#ifdef Asserts
template <class Data>
peano::heap::Heap<Data>::SendReceiveTask::SendReceiveTask():
  _rank(-1),
  _data(0) {
}
#endif


template <class Data>
void peano::heap::Heap<Data>::switchReceiveAndDeployBuffer(std::map<int,int>  sendStatistics) {
  assertionEquals(getSizeOfDeployBuffer(),0);

  int sumOfSentMessages = 0;
  for (std::map<int,int>::const_iterator p=sendStatistics.begin(); p!=sendStatistics.end(); p++) {
    sumOfSentMessages += p->second;
  }

  _currentReceiveBuffer = 1-_currentReceiveBuffer;
  assertionEquals( _neighbourDataReceiveTasks[_currentReceiveBuffer].size(),0 );

  if (sumOfSentMessages < static_cast<int>(_neighbourDataReceiveTasks[1-_currentReceiveBuffer].size())) {
    logDebug( "switchReceiveAndDeployBuffer(map)", "we have to copy back messages from the deploy buffer to the receive buffer" );

    for (
      typename std::vector<SendReceiveTask>::iterator p = _neighbourDataReceiveTasks[1-_currentReceiveBuffer].begin();
      p != _neighbourDataReceiveTasks[1-_currentReceiveBuffer].end();
    ) {
      if ( sendStatistics.count(p->_rank)>0 && sendStatistics[p->_rank]>0 ) {
        sendStatistics[p->_rank]--;
        p++;
      }
      else {
        _neighbourDataReceiveTasks[_currentReceiveBuffer].push_back(*p);
        p = _neighbourDataReceiveTasks[1-_currentReceiveBuffer].erase(p);
      }
    }

    for (std::map<int,int>::const_iterator p=sendStatistics.begin(); p!=sendStatistics.end(); p++) {
      assertionEquals(p->second, 0);
    }
  }

  assertionEquals( static_cast<int>(_neighbourDataReceiveTasks[1-_currentReceiveBuffer].size()), sumOfSentMessages);
}


template <class Data>
void peano::heap::Heap<Data>::startToSendOrReceiveHeapData(bool isTraversalInverted) {
  logTraceInWith5Arguments( "startToSendOrReceiveHeapData()", _name, _neighbourDataSendTasks.size(), _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );

  const std::map<int,int> sendStatistics = releaseSentNeighbourMessages();
  _neighbourDataSendTasks.clear();

  waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages(sendStatistics);

  releaseReceivedNeighbourMessagesRequests();

  // either deploy buffer is empty or receive buffer is empty
  assertion3(
    _neighbourDataReceiveTasks[_currentReceiveBuffer].empty() || _neighbourDataReceiveTasks[1-_currentReceiveBuffer].empty(),
    _neighbourDataReceiveTasks[0].size(),
    _neighbourDataReceiveTasks[1].size(),
    _currentReceiveBuffer
  );


  _readDeployBufferInReverseOrder                         = _wasTraversalInvertedThroughoutLastSendReceiveTraversal != isTraversalInverted;
  _wasTraversalInvertedThroughoutLastSendReceiveTraversal = isTraversalInverted;
  _heapIsCurrentlySentReceived                            = true;

  switchReceiveAndDeployBuffer(sendStatistics);

  logTraceOutWith5Arguments( "startToSendOrReceiveHeapData()", _readDeployBufferInReverseOrder, _neighbourDataSendTasks.size(), _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );

}


template <class Data>
void peano::heap::Heap<Data>::finishedToSendOrReceiveHeapData() {
  logTraceInWith5Arguments( "finishedToSendOrReceiveHeapData()", _name, _neighbourDataSendTasks.size(), _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );

  releaseAndClearSentForkJoinAndMasterWorkerMessages();
  _heapIsCurrentlySentReceived = false;

  logTraceOutWith5Arguments( "finishedToSendOrReceiveHeapData()", _readDeployBufferInReverseOrder, _neighbourDataSendTasks.size(), _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );
}
