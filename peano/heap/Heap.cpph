#include <limits>
#include <memory.h> 

#ifdef Parallel
#include <mpi.h>
#endif

#include "tarch/Assertions.h"
#include "tarch/multicore/Lock.h"
#include "tarch/services/ServiceRepository.h"


template <class Data>
tarch::logging::Log peano::heap::Heap<Data>::_log("peano::heap::Heap");


template <class Data>
peano::heap::Heap<Data>::Heap()
  : _heapData(), _freedHeapIndices(), _nextIndex(0)
  #ifdef Parallel
    , _mpiTagForNeighbourDataExchange(tarch::parallel::Node::reserveFreeTag("heap[neighbour]"))
    , _mpiTagForMasterWorkerDataExchange(tarch::parallel::Node::reserveFreeTag("heap[master-worker]"))
    , _mpiTagToExchangeForkJoinData(tarch::parallel::Node::reserveFreeTag("heap[join/fork]"))
  #endif
  , _neighbourDataSendTasks()
  ,_currentReceiveBuffer(0)
  #if defined(SharedMemoryParallelisation)
    , _dataSemaphore()
  #endif
  ,_readDeployBufferInReverseOrder(true)
  ,_wasTraversalInvertedThroughoutLastSendReceiveTraversal(false)
  ,_heapIsCurrentlySentReceived(false)
{
  #ifdef Parallel
  if (SendReceiveTask::MPIData::Datatype==0) {
    SendReceiveTask::MPIData::initDatatype();
  }
  if(MetaInformation::Datatype==0) {
    MetaInformation::initDatatype();
  }
  #endif

  tarch::services::ServiceRepository::getInstance().addService( this, "peano::heap::Heap" );
}


template <class Data>
peano::heap::Heap<Data>::~Heap()
{
  deleteAllData();
}


template <class Data>
void peano::heap::Heap<Data>::deleteAllData() {
  for(typename std::map<int, std::vector<Data>*>::iterator i = _heapData.begin(); i != _heapData.end(); i++) {
    assertionMsg((*i).second != 0, _name << ": Null-pointer was stored in heap data map.");
    delete (*i).second;
  }
  _heapData.clear();
}


template <class Data>
void peano::heap::Heap<Data>::plotStatistics() {
  if(_name != "") {
    logInfo("plotStatistics()", "Statistics for " << _name);
  }
  logInfo("plotStatistics()", "size of heap: " << _heapData.size() << " entries" );
  logInfo("plotStatistics()", "freed but not reassigned heap indices: " << _freedHeapIndices.size() );

  logInfo("plotStatistics()", "maximum number of allocated heap entries: " << _maximumNumberOfHeapEntries );
  logInfo("plotStatistics()", "number of heap allocations: " << _numberOfHeapAllocations );
  logInfo("plotStatistics()", "number of heap frees: " << _numberOfHeapFrees );

  logInfo("plotStatistics()", "records sent to neighbours: " << _numberOfRecordsSentToNeighbour << " (" << (_numberOfRecordsSentToNeighbour * sizeof(Data)) <<  " Byte(s)) in " << _numberOfMessagesSentToNeighbour << " message(s)" );
  logInfo("plotStatistics()", "records sent to master and workers: " << _numberOfRecordsSentToMasterWorker << " (" << (_numberOfRecordsSentToMasterWorker * sizeof(Data)) <<  " Byte(s)) in " << _numberOfMessagesSentToMasterWorker << " message(s)" );
  logInfo("plotStatistics()", "records sent due to join or fork: " << _numberOfRecordsSentDueToForkJoin << " (" << (_numberOfRecordsSentDueToForkJoin * sizeof(Data)) <<  " Byte(s)) in " << _numberOfMessagesSentDueToForkJoin << " message(s)" );

  logInfo("plotStatistics()", "messages received from neighbours: "         << _numberOfMessagesReceivedFromNeighbour  );
  logInfo("plotStatistics()", "messages received from master and workers: " << _numberOfMessagesReceivedFromMasterWorker  );
  logInfo("plotStatistics()", "messages received due to join or fork: "     << _numberOfMessagesReceivedDueToForkJoin  );
}


template <class Data>
void peano::heap::Heap<Data>::clearStatistics() {
  _maximumNumberOfHeapEntries   = 0;
  _numberOfHeapAllocations      = 0;
  _numberOfHeapFrees            = 0;

  _numberOfRecordsSentToNeighbour    = 0;
  _numberOfRecordsSentToMasterWorker = 0;
  _numberOfRecordsSentDueToForkJoin  = 0;

  _numberOfMessagesSentToNeighbour    = 0;
  _numberOfMessagesSentToMasterWorker = 0;
  _numberOfMessagesSentDueToForkJoin  = 0;

  _numberOfMessagesReceivedFromNeighbour    = 0;
  _numberOfMessagesReceivedFromMasterWorker = 0;
  _numberOfMessagesReceivedDueToForkJoin    = 0;
}


template <class Data>
peano::heap::Heap<Data>& peano::heap::Heap<Data>::getInstance() {
  static peano::heap::Heap<Data> instance;
  return instance;
}


template <class Data>
int peano::heap::Heap<Data>::createData() {
  logTraceIn( "createData()" );
  #if defined(SharedMemoryParallelisation)
  tarch::multicore::Lock lock(_dataSemaphore);
  #endif

  int index;
  if (_freedHeapIndices.empty()) {
    index = _nextIndex;
    _nextIndex++;
  }
  else {
    index = _freedHeapIndices.front();
    _freedHeapIndices.pop_front();
  }

  assertionMsg(_heapData.find(index) == _heapData.end(), "heap entry does exist already.");
  _heapData.insert( typename HeapContainer::value_type(index,new std::vector<Data>()) );

  assertionMsg(_heapData.find(index) != _heapData.end(), "insertion of heap data not successful.");
  assertion(index >= 0);

  _numberOfHeapAllocations += 1.0;

  if(static_cast<int>(_heapData.size()) > _maximumNumberOfHeapEntries) {
    _maximumNumberOfHeapEntries = static_cast<int>(_heapData.size());
  }

  logTraceOutWith2Arguments("createData()", index, _numberOfHeapAllocations);
  return index;
}


template <class Data>
std::vector<Data>& peano::heap::Heap<Data>::getData(int index) {
  #ifdef Asserts
  std::string message = "Trying to get heap data for unknown index. Has the index been initialized correctly?";
  #endif
  assertion4(_heapData.find(index) != _heapData.end(), _name, message, index, _heapData.size());
  assertionMsg(_heapData[index] != 0, _name << ": Null-pointer was stored in heap data map");
  return *(_heapData[index]);
}


template <class Data>
const std::vector<Data>& peano::heap::Heap<Data>::getData(int index) const {
  #ifdef Asserts
  std::string message = "Trying to get heap data for unknown index. Has the index been initialized correctly?";
  #endif
  assertion4(_heapData.find(index) != _heapData.end(), _name, message, index, _heapData.size());
  assertionMsg(_heapData[index] != 0, _name << ": Null-pointer was stored in heap data map");
  return *(_heapData[index]);
}


template <class Data>
void peano::heap::Heap<Data>::moveData( int toIndex, int fromIndex ) {
  assertion4(_heapData.find(toIndex) != _heapData.end(), _name, toIndex, fromIndex, _heapData.size());
  assertion4(_heapData.find(fromIndex) != _heapData.end(), _name, toIndex, fromIndex, _heapData.size());

  _heapData[toIndex]->insert( _heapData[toIndex]->end(), _heapData[fromIndex]->begin(), _heapData[fromIndex]->end() );
  _heapData[fromIndex]->clear();
}


template <class Data>
bool peano::heap::Heap<Data>::isValidIndex(int index) const {
    return _heapData.find(index) != _heapData.end();
}


template <class Data>
void peano::heap::Heap<Data>::deleteData(int index) {
  logTraceInWith2Arguments("deleteData(int)", _name, index);

  #if defined(SharedMemoryParallelisation)
  tarch::multicore::Lock lock(_dataSemaphore);
  #endif
  #ifdef Asserts
  std::string message = "Trying to get heap data for unknown index. Has the index been initialized correctly?";
  #endif
  assertion4(_heapData.find(index) != _heapData.end(), _name, message, index, _heapData.size());
  assertionMsg(_heapData[index] != 0, _name << ": Null-pointer was stored in heap data map");

  _heapData[index]->clear();
  delete _heapData[index];
  _heapData.erase(index);

  _freedHeapIndices.push_back(index);
  _numberOfHeapFrees++;

  logTraceOut("deleteData(int)");
}


template <class Data>
int peano::heap::Heap<Data>::getNumberOfAllocatedEntries() const {
  return _heapData.size();
}


template <class Data>
void peano::heap::Heap<Data>::restart() {
  deleteAllData();
}


template <class Data>
void peano::heap::Heap<Data>::shutdown() {
  deleteAllData();
}


template <class Data>
void peano::heap::Heap<Data>::setName(std::string name) {
  _name = name;
}


template <class Data>
void peano::heap::Heap<Data>::sendData(
  const std::vector< Data >&                    data,
  int                                           toRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  logTraceInWith6Arguments( "sendData(...)", _name, toRank, position, level, messageType, data.size() );

  #ifdef Parallel
  int mpiTag = getTagForMessageType(messageType,true,data.size());

  SendReceiveTask sendTask;

  sendTask._data = new typename SendReceiveTask::MPIData[data.size()];
  sendTask._rank = toRank;

  for (int i=0; i<static_cast<int>( data.size() ); i++) {
    #if defined(ParallelExchangePackedRecords)
    sendTask._data[i] = data[i].convert();
    #else
    sendTask._data[i] = data[i];
    #endif
  }

  #ifdef Asserts
  //Set debug information
  sendTask._metaInformation.setPosition(position);
  sendTask._metaInformation.setLevel(level);
  #endif

  logDebug("sendData", "Sending data at " << position << " to Rank " << toRank << " with mpiTag: " << mpiTag << ", neighbour-data-tag: " << _mpiTagForNeighbourDataExchange << ", master/worker-tag=" << _mpiTagForMasterWorkerDataExchange << ", fork/join-tag=" << _mpiTagToExchangeForkJoinData);

  sendTask._metaInformation.setLength(static_cast<int>( data.size() ));
  sendTask._metaInformation.send(toRank, mpiTag, true);

  assertion((messageType==ForkOrJoinCommunication) || (messageType==MasterWorkerCommunication) || _heapIsCurrentlySentReceived);

  int result = MPI_Isend(
    sendTask._data, sendTask._metaInformation.getLength(), SendReceiveTask::MPIData::Datatype, toRank,
    mpiTag,
    tarch::parallel::Node::getInstance().getCommunicator(), &sendTask._request
  );

  if ( result != MPI_SUCCESS ) {
    logError(
      "sendData(...)", "failed to send heap data to node "
      << toRank << ": " << tarch::parallel::MPIReturnValueToString(result)
    );
  }

  if (messageType == NeighbourCommunication) {
    _neighbourDataSendTasks.push_back(sendTask);
  }
  else if (messageType == MasterWorkerCommunication) {
    _masterWorkerDataSendTasks.push_back(sendTask);
  } else {
    _forkJoinDataSendTasks.push_back(sendTask);
  }
  #endif

  logTraceOut( "sendData(...)" );
}


template <class Data>
void peano::heap::Heap<Data>::sendData(
  int                                           index,
  int                                           toRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  logTraceInWith6Arguments( "sendData(...)", _name, index, toRank, position, level, messageType );

  sendData( getData(index), toRank, position, level, messageType );

  logTraceOutWith1Argument( "sendData(...)", getData(index).size() );
}


template <class Data>
void peano::heap::Heap<Data>::releaseSentMessages(std::vector<SendReceiveTask>& tasks) {
  #ifdef Parallel
  logTraceInWith1Argument( "releaseSentMessages()", tasks.size() );

  for(typename std::vector<SendReceiveTask>::iterator i = tasks.begin(); i != tasks.end(); ++i) {
    MPI_Status status;

    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;
    int            finishedWait            = false;

    while (!finishedWait) {
      MPI_Test(&(i->_request), &finishedWait, &status);

      // deadlock aspect
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::Heap",
           "releaseSentMessages()", i->_rank,-1,i->_metaInformation.getLength()
         );
         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
         std::ostringstream msg;
         msg << "tag either " << _mpiTagForNeighbourDataExchange
             << ", " << _mpiTagForMasterWorkerDataExchange
             << " or " << _mpiTagToExchangeForkJoinData;
         tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::Heap",
           "releaseSentMessages()", i->_rank,
           -1,
           i->_metaInformation.getLength(), msg.str()
         );
      }
      tarch::parallel::Node::getInstance().receiveDanglingMessages();
    }

    delete[] (i->_data);
  }
  logTraceOut( "releaseSentMessages()" );
  #endif
}


template <class Data>
void peano::heap::Heap<Data>::releaseSentNeighbourMessages() {
  #ifdef Parallel
  logTraceInWith3Arguments( "releaseSentNeighbourMessages()", _neighbourDataSendTasks.size(), _masterWorkerDataSendTasks.size(), _forkJoinDataSendTasks.size() );

  releaseSentMessages( _neighbourDataSendTasks );

  logTraceOut( "releaseSentNeighbourMessages()" );
  #endif
}


template <class Data>
void peano::heap::Heap<Data>::releaseAndClearSentForkJoinAndMasterWorkerMessages() {
  #ifdef Parallel
  logTraceInWith3Arguments( "releaseAndClearSentForkJoinAndMasterWorkerMessages()", _neighbourDataSendTasks.size(), _masterWorkerDataSendTasks.size(), _forkJoinDataSendTasks.size() );

  releaseSentMessages( _masterWorkerDataSendTasks );
  releaseSentMessages( _forkJoinDataSendTasks );

  _masterWorkerDataSendTasks.clear();
  _forkJoinDataSendTasks.clear();

  logTraceOut( "releaseAndClearSentForkJoinAndMasterWorkerMessages()" );
  #endif
}



template <class Data>
void peano::heap::Heap<Data>::waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages() {
  logTraceInWith3Arguments( "waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages()", _name, getSizeOfReceiveBuffer(), getSizeOfSendBuffer() );

  #ifdef Parallel
  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;
  while (getSizeOfReceiveBuffer()<getSizeOfSendBuffer()) {
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::Heap::releaseMessages",
         "waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages()", -1,
         _mpiTagForNeighbourDataExchange, -1
       );

       logDebug("waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages", "send=" << getSizeOfSendBuffer() << ", receive=" << getSizeOfReceiveBuffer());

       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::Heap::releaseMessages",
         "waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages()", -1,
         _mpiTagForNeighbourDataExchange, -1
       );
    }

    tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }
  #endif

  logTraceOutWith3Arguments( "waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages()", _name, getSizeOfReceiveBuffer(), getSizeOfSendBuffer() );
}


template <class Data>
void peano::heap::Heap<Data>::releaseReceivedNeighbourMessagesRequests() {
  #ifdef Parallel
  // I originally used a plain for loop over the receive buffer, but that does not
  // work, as a receiveDanglingMessages within this for loop might change the
  //  underlying  data container.
  bool allMessageCommunicationsAreFinished = false;

  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;
  int            finishedWait            = false;

  while (!allMessageCommunicationsAreFinished) {
    allMessageCommunicationsAreFinished = true;
    for(typename std::vector<SendReceiveTask>::iterator i = _neighbourDataReceiveTasks[_currentReceiveBuffer].begin(); i != _neighbourDataReceiveTasks[_currentReceiveBuffer].end(); ++i) {
      MPI_Status status;
      MPI_Test(&(i->_request), &finishedWait, &status);
      allMessageCommunicationsAreFinished &= (finishedWait!=0);
    }

    // deadlock aspect
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::Heap",
         "releaseReceivedMessagesRequests()", -1,
         _mpiTagForNeighbourDataExchange, -1
       );
       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::Heap",
         "releaseReceivedMessagesRequests()", -1,
         _mpiTagForNeighbourDataExchange, -1
       );
    }

    tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }
  #endif
}


template <class Data>
void peano::heap::Heap<Data>::removeMessageFromBuffer(int number, std::vector<SendReceiveTask>& tasks) {
  assertion( number>=0 );
  assertion( number<static_cast<int>(tasks.size()) );

  typename std::vector<SendReceiveTask>::iterator it = tasks.begin();
  it+=number;

  delete[] it->_data;
  tasks.erase(it);
}


template <class Data>
int peano::heap::Heap<Data>::findMessageFromRankInNeighbourDataDeployBuffer(int ofRank) const {
  const int currentDeployBuffer = 1-_currentReceiveBuffer;
  if (_readDeployBufferInReverseOrder) {
    for (int i=getSizeOfDeployBuffer()-1; i>=0; i--) {
      if (_neighbourDataReceiveTasks[currentDeployBuffer][i]._rank==ofRank) {
        return i;
      }
    }
  }
  else {
    for (int i=0; i<getSizeOfDeployBuffer(); i++) {
      if (_neighbourDataReceiveTasks[currentDeployBuffer][i]._rank==ofRank) {
        return i;
      }
    }
  }
  assertionMsg( false, "no fitting message found in neighbour data buffer" );
  return -1;
}


template <class Data>
int peano::heap::Heap<Data>::findMessageFromRankInMasterWorkerOrForkJoinDataBuffer(
  int                           ofRank,
  std::vector<SendReceiveTask>& tasks
) {
  #ifdef Parallel
  for (int i=0; i<static_cast<int>(tasks.size()); i++) {
    if (tasks[i]._rank==ofRank) {
      MPI_Status status;
      int        finishedWait = false;
      MPI_Test(&(tasks[i]._request), &finishedWait, &status);

      if (finishedWait) {
        return i;
      }
      else {
        return -1;
      }
    }
  }
  #endif
  return -1;
}

template <class Data>
std::vector< Data > peano::heap::Heap<Data>::receiveMasterWorkerOrForkJoinData(
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  logTraceInWith1Argument( "receiveMasterWorkerOrForkJoinData(...)", fromRank );

  assertion(messageType == MasterWorkerCommunication || messageType == ForkOrJoinCommunication);
  assertion(messageType!=NeighbourCommunication);

  std::vector< Data >  result;
  #ifdef Parallel
  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;
  int            resultTask              = -1;

  int mpiTag = getTagForMessageType(messageType,false,0);

  std::vector<SendReceiveTask>& receiveTasks = (messageType == MasterWorkerCommunication) ? _masterWorkerDataReceiveTasks : _forkJoinDataReceiveTasks;

  logDebug("receiveMasterWorkerOrForkJoinData", "Receiving data synchronously with mpiTag: " << mpiTag << ", fork/join-tag=" << _mpiTagToExchangeForkJoinData << ", neighbour-data-tag=" << _mpiTagForNeighbourDataExchange << ", master/worker-tag=" <<  _mpiTagForMasterWorkerDataExchange);

  while (resultTask == -1) {
    resultTask = findMessageFromRankInMasterWorkerOrForkJoinDataBuffer(fromRank, receiveTasks);

    // deadlock aspect
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::Heap",
         "receiveMasterWorkerOrForkJoinData()", fromRank,
         mpiTag, -1
       );
       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::Heap",
         "receiveMasterWorkerOrForkJoinData()", fromRank,
         mpiTag, -1
       );
    }
    tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }

  result = extractMessageFromTaskQueue(resultTask, receiveTasks, position, level);
  removeMessageFromBuffer( resultTask, receiveTasks );
  #endif

  logTraceOutWith1Argument( "receiveMasterWorkerOrForkJoinData(...)", result.size() );
  return result;
}

template <class Data>
std::vector< Data > peano::heap::Heap<Data>::extractMessageFromTaskQueue(
  int                                           messageNumber,
  std::vector<SendReceiveTask>&                 tasks,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  std::vector< Data > result;

  #ifdef Parallel

  const int numberOfElementsOfThisEntry = tasks[messageNumber]._metaInformation.getLength();

  assertionEquals4(
    tasks[messageNumber]._metaInformation.getLevel(),
    level,
    tasks[messageNumber]._metaInformation.toString(),
    level,  position,
    tarch::parallel::Node::getInstance().getRank()
  );
  for (int d=0; d<DIMENSIONS; d++) {
    assertionNumericalEquals5(
      tasks[messageNumber]._metaInformation.getPosition(d),
      position(d),
      tasks[messageNumber]._metaInformation.toString(),
      level,  position,  d,
      tarch::parallel::Node::getInstance().getRank()
    );
  }

  assertion(tasks[messageNumber]._data!=0);
  for (int i=0; i<numberOfElementsOfThisEntry; i++) {
    #if defined(ParallelExchangePackedRecords)
    result.push_back(tasks[messageNumber]._data[i].convert());
    #else
    result.push_back(tasks[messageNumber]._data[i]);
    #endif
  }

  #endif

  return result;
}


template <class Data>
std::vector< Data > peano::heap::Heap<Data>::receiveNeighbourData(
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  logTraceInWith6Arguments( "receiveNeighbourData(...)", fromRank, position, level, _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );

  std::vector< Data > result;

  #ifdef Parallel
  assertion1(_heapIsCurrentlySentReceived, tarch::parallel::Node::getInstance().getRank());
  assertion3( getSizeOfDeployBuffer()>0, tarch::parallel::Node::getInstance().getRank(), toString(), "if the neighbour data buffer is empty, you have perhaps forgotten to call releaseMessages() on the heap in the traversal before" );

  const int currentDeployBuffer         = 1-_currentReceiveBuffer;
  const int elementFromDeployBuffer     = findMessageFromRankInNeighbourDataDeployBuffer(fromRank);

  result = extractMessageFromTaskQueue(elementFromDeployBuffer,_neighbourDataReceiveTasks[currentDeployBuffer],position,level);
  removeMessageFromBuffer( elementFromDeployBuffer, _neighbourDataReceiveTasks[currentDeployBuffer] );
  #endif

  logTraceOutWith1Argument( "receiveNeighbourData(...)", result.size() );
  return result;
}


template <class Data>
std::vector< Data > peano::heap::Heap<Data>::receiveData(
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  if (messageType == NeighbourCommunication) {
    return receiveNeighbourData(fromRank, position, level);
  } else {
    return receiveMasterWorkerOrForkJoinData(fromRank, position, level, messageType);
  }
}



template <class Data>
void peano::heap::Heap<Data>::receiveData(
  int                                           index,
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level,
  MessageType                                   messageType
) {
  std::vector< Data >   receivedData;
  std::vector< Data >&  localData = getData(index);
  if (messageType == NeighbourCommunication) {
    receivedData = receiveNeighbourData(fromRank, position, level);
  } else {
    receivedData = receiveMasterWorkerOrForkJoinData(fromRank, position, level, messageType);
  }
  localData.insert( localData.end(), receivedData.begin(), receivedData.end() );
}


template <class Data>
void peano::heap::Heap<Data>::receiveDanglingMessages(int tag, std::vector<SendReceiveTask>& taskQueue) {
  #ifdef Parallel
  int        flag   = 0;
  MPI_Status status;
  int        result = MPI_Iprobe(
    MPI_ANY_SOURCE,
    tag,
    tarch::parallel::Node::getInstance().getCommunicator(),
    &flag,
    &status
  );
  if (result!=MPI_SUCCESS) {
    logError(
      "receiveDanglingMessages()",
      "probing for messages failed: " << tarch::parallel::MPIReturnValueToString(result)
    );
  }
  if (flag) {
    logTraceInWith1Argument( "receiveDanglingMessages(...)", tag );

    SendReceiveTask receiveTask;

    receiveTask._metaInformation.receive(status.MPI_SOURCE, tag, true);

    receiveTask._data = new typename SendReceiveTask::MPIData[ receiveTask._metaInformation.getLength() ];
    receiveTask._rank = status.MPI_SOURCE;

    result = MPI_Irecv(receiveTask._data, receiveTask._metaInformation.getLength(), SendReceiveTask::MPIData::Datatype, status.MPI_SOURCE, tag, tarch::parallel::Node::getInstance().getCommunicator(), &receiveTask._request);
    if ( result != MPI_SUCCESS ) {
      logError(
        "receiveDanglingMessages()",
        "failed to receive heap data from node "
        << status.MPI_SOURCE << ": " << tarch::parallel::MPIReturnValueToString(result)
      );
    }

    taskQueue.push_back( receiveTask );

    logDebug(
      "receiveDanglingMessages(...)",
      "started to receive " << taskQueue.size() << "th message from rank " << receiveTask._rank << " with " << receiveTask._metaInformation.getLength() << " entries and data pointing to " << receiveTask._data
    );

    logTraceOutWith3Arguments( "receiveDanglingMessages(...)", _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );
  }
  #endif
}


template <class Data>
void peano::heap::Heap<Data>::receiveDanglingMessages() {
  #ifdef Parallel
  receiveDanglingMessages(_mpiTagToExchangeForkJoinData , _forkJoinDataReceiveTasks);
  receiveDanglingMessages(_mpiTagForNeighbourDataExchange, _neighbourDataReceiveTasks[_currentReceiveBuffer]);
  receiveDanglingMessages(_mpiTagForMasterWorkerDataExchange, _masterWorkerDataReceiveTasks);
  #endif
}

template <class Data>
int peano::heap::Heap<Data>::getTagForMessageType(MessageType messageType, bool isSendTask, int messageSize) {
  #ifdef Parallel
    switch(messageType) {
      case NeighbourCommunication:
        if (isSendTask) {
          _numberOfRecordsSentToNeighbour  += messageSize;
          _numberOfMessagesSentToNeighbour += 1;
        }
        else {
          _numberOfMessagesReceivedFromNeighbour += 1;
        }
        return _mpiTagForNeighbourDataExchange;
      case MasterWorkerCommunication:
        if (isSendTask) {
          _numberOfRecordsSentToMasterWorker  += messageSize;
          _numberOfMessagesSentToMasterWorker += 1;
        }
        else {
          _numberOfMessagesReceivedFromMasterWorker += 1;
        }
        return _mpiTagForMasterWorkerDataExchange;
      case ForkOrJoinCommunication:
        if (isSendTask) {
          _numberOfRecordsSentDueToForkJoin  += messageSize;
          _numberOfMessagesSentDueToForkJoin += 1;
        }
        else {
          _numberOfMessagesReceivedDueToForkJoin += 1;
        }
        return _mpiTagToExchangeForkJoinData;
      default:
        assertionMsg( false, "not supported" );
        return -1;
    }
  #else
  return -1;
  #endif
}

template <class Data>
int peano::heap::Heap<Data>::getSizeOfReceiveBuffer() const {
  return static_cast<int>(_neighbourDataReceiveTasks[_currentReceiveBuffer].size());
}


template <class Data>
int peano::heap::Heap<Data>::getSizeOfDeployBuffer() const {
  return static_cast<int>(_neighbourDataReceiveTasks[1-_currentReceiveBuffer].size());
}


template <class Data>
int peano::heap::Heap<Data>::getSizeOfSendBuffer() const {
  return static_cast<int>(_neighbourDataSendTasks.size());
}


template <class Data>
std::string peano::heap::Heap<Data>::toString() const {
  std::ostringstream msg;

  msg << "(name=" << _name
      << ",heap-map-size=" << _heapData.size();

  #ifdef Parallel
  msg << ",mpi-tag-neighbour-data=" << _mpiTagForNeighbourDataExchange
      << ",mpit-tag-master-worker-data=" << _mpiTagForMasterWorkerDataExchange
      << ",mpi-tag-join/fork-data=" << _mpiTagToExchangeForkJoinData
      << ",read-deploy-buffer-in-reverse-order=" << _readDeployBufferInReverseOrder
      << ",current-receive-buffer=" << _currentReceiveBuffer
      << ",unfinished-send-tasks=" << _neighbourDataSendTasks.size()
      << ",neighbour-data-receive-buffer-size-0=" << _neighbourDataReceiveTasks[0].size()
      << ",neighbour-data-receive-buffer-size-1=" << _neighbourDataReceiveTasks[1].size();
  #endif

  msg << ")";

  return msg.str();
}


#ifdef Asserts
template <class Data>
peano::heap::Heap<Data>::SendReceiveTask::SendReceiveTask():
  _rank(-1),
  _data(0) {
}
#endif


template <class Data>
void peano::heap::Heap<Data>::startToSendOrReceiveHeapData(bool isTraversalInverted) {
  logTraceInWith5Arguments( "startToSendOrReceiveHeapData()", _name, _neighbourDataSendTasks.size(), _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );

  #ifdef Parallel
  releaseSentNeighbourMessages();
  releaseAndClearSentForkJoinAndMasterWorkerMessages();

  waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages();
  _neighbourDataSendTasks.clear();

  releaseReceivedNeighbourMessagesRequests();

  // either deploy buffer is empty or receive buffer is empty
  assertion3(
    _neighbourDataReceiveTasks[_currentReceiveBuffer].empty() || _neighbourDataReceiveTasks[1-_currentReceiveBuffer].empty(),
    _neighbourDataReceiveTasks[0].size(),
    _neighbourDataReceiveTasks[1].size(),
    _currentReceiveBuffer
  );

  _readDeployBufferInReverseOrder                         = _wasTraversalInvertedThroughoutLastSendReceiveTraversal != isTraversalInverted;
  _wasTraversalInvertedThroughoutLastSendReceiveTraversal = isTraversalInverted;
  _heapIsCurrentlySentReceived                            = true;

  if (getSizeOfDeployBuffer()==0) {
	  _currentReceiveBuffer                               = 1-_currentReceiveBuffer;
	  _neighbourDataReceiveTasks[_currentReceiveBuffer].clear();
  }
#endif
  logTraceOutWith5Arguments( "startToSendOrReceiveHeapData()", _readDeployBufferInReverseOrder, _neighbourDataSendTasks.size(), _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );

}


template <class Data>
void peano::heap::Heap<Data>::finishedToSendOrReceiveHeapData() {
  logTraceInWith5Arguments( "finishedToSendOrReceiveHeapData()", _name, _neighbourDataSendTasks.size(), _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );

  _heapIsCurrentlySentReceived = false;

  logTraceOutWith5Arguments( "finishedToSendOrReceiveHeapData()", _readDeployBufferInReverseOrder, _neighbourDataSendTasks.size(), _currentReceiveBuffer, _neighbourDataReceiveTasks[0].size(), _neighbourDataReceiveTasks[1].size() );
}
