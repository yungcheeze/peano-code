#include "tarch/Assertions.h"
#include "tarch/compiler/CompilerSpecificSettings.h"


template<class Data>
tarch::logging::Log  peano::heap::BufferedBoundaryDataExchanger<Data>::_log( "peano::heap::BufferedBoundaryDataExchanger" );


template<class Data>
peano::heap::BufferedBoundaryDataExchanger<Data>::BufferedBoundaryDataExchanger():
  _sizeOfMessages(),
  _concatenatedSentData() {
}


template<class Data>
peano::heap::BufferedBoundaryDataExchanger<Data>::BufferedBoundaryDataExchanger(const std::string& identifier, int tag, int rank):
  BoundaryDataExchanger<Data>(identifier,tag,rank),
  _sizeOfMessages(),
  _concatenatedSentData() {
}


template<class Data>
peano::heap::BufferedBoundaryDataExchanger<Data>::~BufferedBoundaryDataExchanger() {
//  assertion4( _sizeOfMessages.empty(), _sizeOfMessages.size(), BoundaryDataExchanger<Data>::_identifier, BoundaryDataExchanger<Data>::_tag, BoundaryDataExchanger<Data>::_rank );
}


template<class Data>
int peano::heap::BufferedBoundaryDataExchanger<Data>::getNumberOfSentMessages() const {
  return static_cast<int>( _sizeOfMessages.size() > 0 ? _sizeOfMessages.size()-1 : 0 );
}


template<class Data>
void peano::heap::BufferedBoundaryDataExchanger<Data>::postprocessFinishedToSendData() {
  logTraceInWith2Arguments( "postprocessFinishedToSendData()", _sizeOfMessages.size(), _concatenatedSentData.size() );

  assertion( Base::_sendTasks.empty() );
  assertion4( Base::_rank >=0, Base::_rank, Base::_identifier, Base::_tag, _concatenatedSentData.size() );

  Base::_sendTasks.push_back( SendReceiveTask<Data>() );
  Base::_sendTasks.back()._metaInformation.setLength( 0 );
  Base::_sendTasks.back()._data = nullptr;
  Base::_sendTasks.back()._metaInformation.send(Base::_rank, Base::_tag, true, SendAndReceiveHeapMetaDataBlocking);
  logDebug( "postprocessFinishedToSendData()", "deployed one dummy send task=" << Base::_sendTasks.back().toString() );

  _sizeOfMessages.push_back(_concatenatedSentData.size());
  int tmp   = _sizeOfMessages.size();
  int result = MPI_Send( &tmp, 1, MPI_INT, Base::_rank, Base::_tag, tarch::parallel::Node::getInstance().getCommunicator() );
  logDebug( "postprocessFinishedToSendData()", "sent header=" << _sizeOfMessages.size() << " identifying number of meta messages (plus one header int)" );

  result = MPI_Send( _sizeOfMessages.data(), _sizeOfMessages.size(), MPI_INT, Base::_rank, Base::_tag, tarch::parallel::Node::getInstance().getCommunicator() );
  if ( result != MPI_SUCCESS ) {
    logError(
       "postprocessFinishedToSendData()", "failed to send heap data to node "
      << Base::_rank << ": " << tarch::parallel::MPIReturnValueToString(result)
    );
  }

  if (  _concatenatedSentData.empty() ) {
    logDebug( "postprocessFinishedToSendData()", "no data exchanged as all meta data records are zero" );
    for (int i=0; i<_sizeOfMessages.size()-1; i++) {
      assertionEquals( _sizeOfMessages[i], 0 );
    }
  }
  else {
    assertion(_sizeOfMessages.size()>0);
    result = MPI_Send( _concatenatedSentData.data(), _concatenatedSentData.size(), MPIData::Datatype, Base::_rank, Base::_tag, tarch::parallel::Node::getInstance().getCommunicator() );
    if ( result != MPI_SUCCESS ) {
      logError(
        "postprocessFinishedToSendData()", "failed to send heap data to node "
        << Base::_rank << ": " << tarch::parallel::MPIReturnValueToString(result)
      );
    }
    logDebug( "postprocessFinishedToSendData()", "sent out data. cardinality=" << _concatenatedSentData.size() );
  }

  logTraceOut( "postprocessFinishedToSendData()" );
}


template<class Data>
void peano::heap::BufferedBoundaryDataExchanger<Data>::postprocessStartToSendData() {
  _sizeOfMessages.clear();
  _concatenatedSentData.clear();
}



template<class Data>
void peano::heap::BufferedBoundaryDataExchanger<Data>::handleAndQueueReceivedTask( const SendReceiveTask<Data>&  receivedTask ) {
  logTraceInWith1Argument( "handleAndQueueReceivedTask(...)", receivedTask.toString() );

  std::vector<int>       sizeOfReceivedMessages;
  std::vector<MPIData>   concatenatedReceivedData;

  assertion5( Base::_rank >=0, Base::_rank, Base::_identifier, Base::_tag, _concatenatedSentData.size(), receivedTask.toString() );
  assertion5( receivedTask._metaInformation.getLength()==0, Base::_rank, Base::_identifier, Base::_tag, _concatenatedSentData.size(), receivedTask.toString() );

  MPI_Status status;
  int        result;

  int numberOfMetaMessages = 0;
  result = MPI_Recv( &numberOfMetaMessages, 1, MPI_INT, Base::_rank, Base::_tag, tarch::parallel::Node::getInstance().getCommunicator(), &status );
  logDebug( "handleAndQueueReceivedTask(...)", "will receive " << numberOfMetaMessages << " meta (int) messages incl. one header int" );

  sizeOfReceivedMessages.resize(numberOfMetaMessages);
  result = MPI_Recv( sizeOfReceivedMessages.data(), numberOfMetaMessages, MPI_INT, Base::_rank, Base::_tag, tarch::parallel::Node::getInstance().getCommunicator(), &status );
  if ( result != MPI_SUCCESS ) {
    logError(
       "postprocessFinishedToSendData()", "failed to send heap data to node "
      << Base::_rank << ": " << tarch::parallel::MPIReturnValueToString(result)
    );
  }

  const int totalNumberOfReceivedData = sizeOfReceivedMessages.back();
  sizeOfReceivedMessages.pop_back();
  if (totalNumberOfReceivedData>0) {
    concatenatedReceivedData.resize(totalNumberOfReceivedData);
    logDebug( "postprocessFinishedToSendData(...)", "start to receive " <<  totalNumberOfReceivedData << " data records through mpi");
    result = MPI_Recv( concatenatedReceivedData.data(), totalNumberOfReceivedData, MPIData::Datatype, Base::_rank, Base::_tag, tarch::parallel::Node::getInstance().getCommunicator(), &status );
    if ( result != MPI_SUCCESS ) {
      logError(
         "postprocessFinishedToSendData()", "failed to send heap data to node "
        << Base::_rank << ": " << tarch::parallel::MPIReturnValueToString(result)
      );
    }
  }
  else {
    logDebug( "postprocessFinishedToSendData(...)", "skip data receive as no data exchanged (only meta data)");
  }

  while (!sizeOfReceivedMessages.empty()) {
    const int numberOfDataRecords = sizeOfReceivedMessages.front();
    sizeOfReceivedMessages.erase(sizeOfReceivedMessages.begin());
    SendReceiveTask<Data> newTask;
    if (numberOfDataRecords>0) {
      newTask._data = new MPIData[numberOfDataRecords];
      newTask._rank = Base::_rank;
      newTask._metaInformation.setLength(numberOfDataRecords);
      for (int i=0; i<numberOfDataRecords; i++) {
        newTask._data[i] = concatenatedReceivedData.front();
        concatenatedReceivedData.erase(concatenatedReceivedData.begin());
      }
    }
    else {
      newTask.setInvalid();
    }
    Base::_receiveTasks[Base::_currentReceiveBuffer].push_back( newTask );
  }

  logTraceOutWith1Argument( "handleAndQueueReceivedTask(...)", Base::_receiveTasks[Base::_currentReceiveBuffer].size() );
}


template<class Data>
void peano::heap::BufferedBoundaryDataExchanger<Data>::handleAndQueueSendTask( const SendReceiveTask<Data>&  sendTask, const std::vector<Data>& data ) {
  logTraceIn( "handleAndQueueSendTask(...)" );

  assertionEquals(sendTask._metaInformation.getLength(), data.size());

  _sizeOfMessages.push_back(data.size());
  #if defined(ParallelExchangePackedRecordsInHeaps)
  for (int i=0; i<static_cast<int>(data.size()); i++ ) {
    _concatenatedSentData.push_back( data[i].convert() );
  }
  #else
  _concatenatedSentData.insert( _bufferedSentData.end(), data );
  #endif


  logTraceOut( "handleAndQueueSendTask(...)" );
}
