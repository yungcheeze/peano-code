#include "peano/datatraversal/ActionSetTraversalLoop.h"
#include "tarch/multicore/MulticoreDefinitions.h"
#include "peano/utils/PeanoOptimisations.h"


template <class LoopBody>
peano::datatraversal::SingleLevelCellLoop<LoopBody>::SingleLevelCellLoop(
  const tarch::la::Vector<DIMENSIONS,int>&  range,
  LoopBody&                                 loopBody,
  int                                       grainSize,
  bool                                      useSixPowerDColouring
) {
  assertion( tarch::la::volume(range)!=0 );

  #if defined(SharedMemoryParallelisation)
  if (grainSize>0 && tarch::la::volume(range)>1 ) {
    runParallel(loopBody,range,grainSize,useSixPowerDColouring);
  }
  else {
    runSequentially(loopBody,range);
  }
  #else
  runSequentially(loopBody,range);
  #endif
}


template <class LoopBody>
void peano::datatraversal::SingleLevelCellLoop<LoopBody>::runSequentially(
  LoopBody&                                 loopBody,
  const tarch::la::Vector<DIMENSIONS,int>&  range
) {
  dfor(currentCell,range) {
    loopBody(currentCell);
  }
}


template <class LoopBody>
void peano::datatraversal::SingleLevelCellLoop<LoopBody>::runParallel(
  LoopBody&                                 loopBody,
  const tarch::la::Vector<DIMENSIONS,int>&  range,
  int                                       grainSize,
  bool                                      useSixPowerDColouring
) {
  for (int d=0; d<DIMENSIONS; d++) {
    assertion2( range(d)>=2, range, grainSize );
  }

  const int colouringAlongOneAxis = useSixPowerDColouring ? 6 : 2;

  dfor(k,colouringAlongOneAxis) {
    tarch::la::Vector<DIMENSIONS,int> localRange = range;
    for (int d=0; d<DIMENSIONS; d++) {
      const int rangeModColouring = localRange(d)%colouringAlongOneAxis;
      if (rangeModColouring!=0 && k(d)<rangeModColouring) {
        localRange(d) = localRange(d) / colouringAlongOneAxis + 1;
      }
      else {
        localRange(d) /= colouringAlongOneAxis;
      }
    }
    dForLoopInstance loopInstance(loopBody,k,colouringAlongOneAxis);
    #ifdef SharedTBB
    tbb::parallel_reduce( dForRange( localRange, grainSize ), loopInstance );
    #elif defined(SharedMemoryParallelisation)
    #error not implemented yet
    #endif
  }
}



template <class LoopBody>
peano::datatraversal::SingleLevelCellLoop<LoopBody>::dForLoopInstance::dForLoopInstance( const LoopBody& loopBody, const tarch::la::Vector<DIMENSIONS,int>& offset, int padding ):
  _loopBody(loopBody),
  _offset(offset),
  _padding(padding) {
  assertion1( _padding>=2, padding );
}


template <class LoopBody>
peano::datatraversal::SingleLevelCellLoop<LoopBody>::dForLoopInstance::dForLoopInstance( const dForLoopInstance& instance, SplitFlag ):
  _loopBody(instance._loopBody),
  _offset(instance._offset),
  _padding(instance._padding) {
  for (int d=0; d<DIMENSIONS; d++) {
    assertion2( _offset(d)>=0, _offset, _padding );
    assertion2( _offset(d)<_padding, _offset, _padding);
  }
}


template <class LoopBody>
void peano::datatraversal::SingleLevelCellLoop<LoopBody>::dForLoopInstance::operator() (const dForRange& range) {
  dfor(i,range.getRange()) {
    _loopBody( (i + range.getOffset())*_padding + _offset );
  }
}


template <class LoopBody>
void peano::datatraversal::SingleLevelCellLoop<LoopBody>::dForLoopInstance::join(const dForLoopInstance&  with) {
}
