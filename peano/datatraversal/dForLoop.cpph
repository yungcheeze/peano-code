#include "peano/datatraversal/dForRange.h"

#include "tarch/multicore/Lock.h"
#include "tarch/la/Vector.h"
#include "peano/utils/Loop.h"
#include "peano/utils/PeanoOptimisations.h"

#include "peano/datatraversal/autotuning/Oracle.h"

#ifdef SharedTBB
#include "tbb/parallel_reduce.h"
#endif

#ifdef SharedOMP
#include <omp.h>
#endif


template <class LoopBody>
tarch::logging::Log peano::datatraversal::dForLoop<LoopBody>::_log( "peano::datatraversal::dForLoop" );


template <class LoopBody>
peano::datatraversal::dForLoop<LoopBody>::dForLoop(
  const tarch::la::Vector<DIMENSIONS,int>&  range,
  LoopBody&                                 body,
  int                                       grainSize,
  bool                                      useSevenPowerDColouring
) {
	assertion( grainSize >= 0 );
	if (grainSize>0) {
    #ifdef SharedTBB
	  if (useSevenPowerDColouring) {
	    assertionMsg( "dForLoop(...)", "not implemented yet" );
	  }
	  else {
      logTraceInWith4Arguments( "dForLoop(...)", range, grainSize, "tbb", "embarassingly parallel" );
      dForLoopInstance loopInstance(body);
      tbb::parallel_reduce(
        dForRange( range, grainSize ),
        loopInstance
      );
	  }
    #elif SharedOMP
    if (useSixPowerDColouring) {
      assertionMsg( "dForLoop(...)", "not implemented yet" );
    }
    else {
      logTraceInWith4Arguments( "dForLoop(...)", range, grainSize, "omp", "embarassingly parallel" );

      dForLoopInstance loopInstance(body);

      std::vector<dForRange> ranges = createRangesVector(range, grainSize);

      #pragma omp parallel for schedule(dynamic, 1) firstprivate(loopInstance)
      for( int i=0; i < (int)(ranges.size()); i++ ){
        loopInstance(ranges[i]);
      }
    }
    #elif SharedCobra
      if (useSixPowerDColouring) {
        assertionMsg( "dForLoop(...)", "not implemented yet" );
      }
      else {
        logTraceInWith4Arguments( "dForLoop(...)", range, grainSize, "cobra", "embarassingly parallel" );

        tarch::multicore::cobra::Core::getInstance().getScheduler().call([&](::cobra::continuator& ctr) {
          realiseParallelForAsTaskBipartitioning(
            peano::datatraversal::dForRange( range, grainSize ),
            ctr,
            dForLoopInstance(body)
          );
        });
      }
    #else
		logTraceInWith3Arguments( "dForLoop(...)", range, grainSize, "no-parallelisation" );
		dfor(i,range) {
			body(i);
		}
    #endif
	}
	else {
		logTraceInWith2Arguments( "dForLoop(...)", range, grainSize );
		dfor(i,range) {
		  body(i);
		}
	}

	logTraceOut( "dForLoop(...)" );
}


#ifdef SharedCobra
template <class LoopBody>
void peano::datatraversal::dForLoop<LoopBody>::realiseParallelForAsTaskBipartitioning(
  peano::datatraversal::dForRange  range,
  ::cobra::continuator&            ctr,
   dForLoopInstance                loopBody
) {
  if (range.is_divisible()) {
    peano::datatraversal::dForRange range1(range);
    peano::datatraversal::dForRange range2(range1,peano::datatraversal::dForRange::Split());
    ctr.fork<0>([=](::cobra::continuator &ctr) {
      realiseParallelForAsTaskBipartitioning(range1, ctr, loopBody);
    });
    ctr.fork<1>([=](::cobra::continuator &ctr) {
      realiseParallelForAsTaskBipartitioning(range2, ctr, loopBody);
    });
    ctr.join([]{});
  }
  // [=] funktioniert hier leider nicht, aber ich bin mir eh nicht sicher, ob man das join hier braucht
//  else ctr.join([&] {
  else {
    loopBody(range);
  }
//  });
}
#endif


template <class LoopBody>
std::vector<peano::datatraversal::dForRange> peano::datatraversal::dForLoop<LoopBody>::createRangesVector(
  const tarch::la::Vector<DIMENSIONS,int>&  range,
  int                                       grainSize
) {
  std::vector<dForRange> ranges;
  ranges.push_back( dForRange( range, grainSize ) );
  bool dividedRange;
  do {
    dividedRange = false;

    int length = static_cast<int>( ranges.size() );
    for(int i = 0; i < length; i++){
      if(ranges[i].is_divisible()){
        ranges.push_back( dForRange( ranges[i], dForRange::Split() ) );
        dividedRange = true;
      }
    }
  } while(dividedRange);

  return ranges;
}


template <class LoopBody>
peano::datatraversal::dForLoop<LoopBody>::dForLoopInstance::dForLoopInstance( const LoopBody& loopBody ):
  _loopBody(loopBody) {
}


template <class LoopBody>
peano::datatraversal::dForLoop<LoopBody>::dForLoopInstance::dForLoopInstance( const dForLoopInstance& loopBody, SplitFlag ):
  _loopBody(loopBody._loopBody) {
}


template <class LoopBody>
void peano::datatraversal::dForLoop<LoopBody>::dForLoopInstance::operator() (const dForRange& range) {
	logTraceInWith1Argument( "dForLoopInstance::operator()", range.toString() );

  dfor(i,range.getRange()) {
	  _loopBody(i + range.getOffset());
	}

  logTraceOutWith1Argument( "dForLoopInstance::operator()", range.toString() );
}


template <class LoopBody>
void peano::datatraversal::dForLoop<LoopBody>::dForLoopInstance::join(const dForLoopInstance&  with) {
}
