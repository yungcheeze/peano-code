#include "peano/datatraversal/dForRange.h"

#include "tarch/multicore/Lock.h"
#include "tarch/la/Vector.h"
#include "peano/utils/Loop.h"
#include "peano/utils/PeanoOptimisations.h"

#include "peano/datatraversal/autotuning/Oracle.h"

#include "peano/datatraversal/TaskSet.h"


#ifdef SharedTBB
#include "tbb/parallel_reduce.h"
#endif

#ifdef SharedOMP
#include <omp.h>
#endif


template <class LoopBody>
tarch::logging::Log peano::datatraversal::dForLoop<LoopBody>::_log( "peano::datatraversal::dForLoop" );


template <class LoopBody>
peano::datatraversal::dForLoop<LoopBody>::dForLoop(
  const tarch::la::Vector<DIMENSIONS,int>&  range,
  LoopBody&                                 body,
  int                                       grainSize
) {
	assertion( grainSize >= 0 );
	if (grainSize>0) {
    #ifdef SharedTBB
      logTraceInWith3Arguments( "dForLoop(...)", range, grainSize, "tbb" );
      #ifdef UseParallelReduceForVertexHandling
      dForLoopInstance loopInstance(body);
	  	tbb::parallel_reduce(
  		  dForRange( range, grainSize ),
  		  loopInstance
  		);
      #else
	  	dForLoopWithBipartitioning(
    	  body, false,
        dForRange( range, grainSize )
	  	);
      #endif
    #elif SharedOMP
	  	logTraceInWith3Arguments( "dForLoop(...)", range, grainSize, "omp" );
		
	  	dForLoopInstance loopInstance(body);

	  	std::vector<dForRange> ranges = createRangesVector(range, grainSize);

      #pragma omp parallel for schedule(dynamic, 1) firstprivate(loopInstance)
      for( int i=0; i < (int)(ranges.size()); i++ ){
        loopInstance(ranges[i]);
      }
    #elif SharedCobra
      logTraceInWith3Arguments( "dForLoop(...)", range, grainSize, "cobra" );

      tarch::multicore::cobra::Core::getInstance().getScheduler().call([&](::cobra::continuator& ctr) {
        realiseParallelForAsTaskBipartitioning(
          peano::datatraversal::dForRange( range, grainSize ),
          ctr,
          dForLoopInstance(body)
        );
      });
    #else
		logTraceInWith3Arguments( "dForLoop(...)", range, grainSize, "no-parallelisation" );
		dfor(i,range) {
			body(i);
		}
    #endif
	}
	else {
		logTraceInWith2Arguments( "dForLoop(...)", range, grainSize );
		dfor(i,range) {
		  body(i);
		}
	}

	logTraceOut( "dForLoop(...)" );
}


#ifdef SharedCobra
template <class LoopBody>
void peano::datatraversal::dForLoop<LoopBody>::realiseParallelForAsTaskBipartitioning(
  peano::datatraversal::dForRange  range,
  ::cobra::continuator&            ctr,
   dForLoopInstance                loopBody
) {
  if (range.is_divisible()) {
    peano::datatraversal::dForRange range1(range);
    peano::datatraversal::dForRange range2(range1,peano::datatraversal::dForRange::Split());
    ctr.fork<0>([=](::cobra::continuator &ctr) {
      realiseParallelForAsTaskBipartitioning(range1, ctr, loopBody);
    });
    ctr.fork<1>([=](::cobra::continuator &ctr) {
      realiseParallelForAsTaskBipartitioning(range2, ctr, loopBody);
    });
    ctr.join([]{});
  }
  // [=] funktioniert hier leider nicht, aber ich bin mir eh nicht sicher, ob man das join hier braucht
//  else ctr.join([&] {
  else {
    loopBody(range);
  }
//  });
}
#endif


template <class LoopBody>
std::vector<peano::datatraversal::dForRange> peano::datatraversal::dForLoop<LoopBody>::createRangesVector(
  const tarch::la::Vector<DIMENSIONS,int>&  range,
  int                                       grainSize
) {
  std::vector<dForRange> ranges;
  ranges.push_back( dForRange( range, grainSize ) );
  bool dividedRange;
  do {
    dividedRange = false;

    int length = static_cast<int>( ranges.size() );
    for(int i = 0; i < length; i++){
      if(ranges[i].is_divisible()){
        ranges.push_back( dForRange( ranges[i], dForRange::Split() ) );
        dividedRange = true;
      }
    }
  } while(dividedRange);

  return ranges;
}


template <class LoopBody>
peano::datatraversal::dForLoop<LoopBody>::dForLoopInstance::dForLoopInstance( const LoopBody& loopBody ):
  _loopBody(loopBody) {
}


template <class LoopBody>
peano::datatraversal::dForLoop<LoopBody>::dForLoopInstance::dForLoopInstance( const dForLoopInstance& loopBody, SplitFlag ):
  _loopBody(loopBody._loopBody) {
}


template <class LoopBody>
void peano::datatraversal::dForLoop<LoopBody>::dForLoopInstance::operator() (const dForRange& range) {
	logTraceInWith1Argument( "dForLoopInstance::operator()", range.toString() );

  dfor(i,range.getRange()) {
	  _loopBody(i + range.getOffset());
	}

  logTraceOutWith1Argument( "dForLoopInstance::operator()", range.toString() );
}


template <class LoopBody>
void peano::datatraversal::dForLoop<LoopBody>::dForLoopInstance::join(const dForLoopInstance&  with) {
}


template <class LoopBody>
peano::datatraversal::dForLoop<LoopBody>::dForLoopWithBipartitioning::dForLoopWithBipartitioning(const LoopBody& loopBody, bool isLeftTask, dForRange range):
  _loopBody(loopBody),
  _isLeftTask(isLeftTask),
  _range(range) {
}


template <class LoopBody>
void peano::datatraversal::dForLoop<LoopBody>::dForLoopWithBipartitioning::operator()() {
  const bool doSplit = !_isLeftTask && _range.is_divisible();

  if (doSplit) {
      dForRange leftRange( _range, dForRange::Split() );
      LoopBody  copyOfLoopBodyForLeftTask(_loopBody);
      dForLoopWithBipartitioning leftTask(
        copyOfLoopBodyForLeftTask, true, leftRange
      );
      dForLoopWithBipartitioning rightTask(
        _loopBody, false, _range
      );
      TaskSet::TaskSet(leftTask,rightTask,true);
    }
    else {
      dfor(i,_range.getRange()) {
        _loopBody(i + _range.getOffset());
      }
    }
}
